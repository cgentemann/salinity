{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the in situ and SSS collocation code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import scipy\n",
    "from glob import glob\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points \n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # convert decimal degrees to radians \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    # haversine formula \n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    # Radius of earth in kilometers is 6371\n",
    "    km = 6371* c\n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in USV data\n",
    "Read in the Saildrone USV file either from a local disc or using OpenDAP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename_usv='f:/data/cruise_data/saildrone/baja-2018/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "def read_usv():\n",
    "    filename_usv='f:/data/cruise_data/saildrone/baja-2018/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc'\n",
    "    ds_usv = xr.open_dataset(filename_usv)\n",
    "    ds_usv.close()\n",
    "    ds_usv = ds_usv.isel(trajectory=0).swap_dims({'obs':'time'}).rename({'longitude':'lon','latitude':'lat'})\n",
    "    ds_usv = ds_usv.sel(time=slice('2018-04-12T02','2018-06-10T18')) #get rid of last part and first part where USV being towed\n",
    "    ds_usv['lon'] = ds_usv.lon.interpolate_na(dim='time',method='linear') #there are 6 nan values\n",
    "    ds_usv['lat'] = ds_usv.lat.interpolate_na(dim='time',method='linear')\n",
    "    ds_usv['wind_speed']=np.sqrt(ds_usv.UWND_MEAN**2+ds_usv.VWND_MEAN**2)\n",
    "    ds_usv['wind_dir']=np.arctan2(ds_usv.VWND_MEAN,ds_usv.UWND_MEAN)*180/np.pi\n",
    "    return ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cruise SSS with coastlines\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(ds_usv.lon, ds_usv.lat, s=3.0, c=ds_usv.SAL_MEAN, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,25,40\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/baja_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is the search code first step.  Narrow down what orbital files to search in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#search each day of USV data for min/max lat/lon \n",
    "#read in orbital files for that day and check to see if any valid data in that box\n",
    "\n",
    "#intialize grid\n",
    "area_def = load_area('areas.cfg', 'pc_world')\n",
    "rlon=np.arange(-180,180,.1)\n",
    "rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "#init filelist\n",
    "file_save=[]\n",
    "\n",
    "#search usv data\n",
    "minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "usv_day = minday\n",
    "print(minday.data,maxday.data)\n",
    "while usv_day<=maxday:\n",
    "    usv_day += np.timedelta64(1,'D')\n",
    "    check_day = np.datetime64(str(usv_day.dt.year.data)+'-'+str(usv_day.dt.month.data).zfill(2)+'-'+str(usv_day.dt.day.data).zfill(2))\n",
    "    usv_day1 = usv_day + np.timedelta64(1,'D')\n",
    "    check_day1 = np.datetime64(str(usv_day1.dt.year.data)+'-'+str(usv_day1.dt.month.data).zfill(2)+'-'+str(usv_day1.dt.day.data).zfill(2))\n",
    "    ds_day = ds_usv.sel(time=slice(check_day,check_day1))\n",
    "    ilen = ds_day.time.size\n",
    "    print(check_day,check_day1,ilen)\n",
    "    if ilen<10:\n",
    "        continue\n",
    "    minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "#caluclate filelist\n",
    "    filelist = glob('F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/'\n",
    "                    +str(usv_day.dt.year.data)+'/'+str(usv_day.dt.dayofyear.data)+'/*.nc')   \n",
    "    x,y,z = [],[],[]\n",
    "    for file in filelist:\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds.close()\n",
    "        x = ds.cellon[:,:,0].data #np.append(x,ds.cellon[:,:,0].data)\n",
    "        y = ds.cellat[:,:,0].data #np.append(y,ds.cellat[:,:,0].data)\n",
    "        z = ds.sss_smap[:,:,0].data #np.append(z,ds.sss_smap[:,:,0].data)\n",
    "        lons,lats,data = x,y,z \n",
    "        lons = np.mod(lons+180,360)-180\n",
    "        swath_def = SwathDefinition(lons, lats)\n",
    "        result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "        da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "        subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "        num_obs = np.isfinite(subset).sum()\n",
    "        if num_obs>0:\n",
    "            file_save = np.append(file_save,file)\n",
    "df = xr.DataArray(file_save,name='filenames')\n",
    "df.to_netcdf('C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/rss40km_filesave.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now read in orbits that have collocated data\n",
    "#subset orbit to region USV has data on that day\n",
    "#need to fill lat/lon nan with value for tree to build\n",
    "#just picked -89 as it is outside usv observation region\n",
    "from scipy import spatial\n",
    "df = xr.open_dataset('C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/rss40km_filesave.nc')\n",
    "for ilook in range(2):\n",
    "    ds_usv = read_usv()\n",
    "    ilen = ds_usv.time.shape[0]\n",
    "    ds_usv['deltaT']=np.ones(ilen)*99999\n",
    "    ds_usv['smap_SSS']=np.ones(ilen)*999999\n",
    "    ds_usv['smap_name']=np.empty(ilen,dtype=str)\n",
    "    ds_usv['smap_ydim']=np.ones(ilen)*999999\n",
    "    ds_usv['smap_xdim']=np.ones(ilen)*999999\n",
    "    for file2 in df.filenames.data:\n",
    "        file = file2\n",
    "        file.replace('\\\\','/')\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds.close()       \n",
    "        lons = ds.cellon[:,:,ilook].fillna(-89).data #np.append(x,ds.cellon[:,:,0].data)\n",
    "        lats = ds.cellat[:,:,ilook].fillna(-89).data #np.append(y,ds.cellat[:,:,0].data)\n",
    "        inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "        tree = spatial.KDTree(inputdata)\n",
    "        orbit_time = ds.time[:,:,0].max().data-np.timedelta64(1,'D')\n",
    "        orbit_time2 = ds.time[:,:,0].max().data+np.timedelta64(1,'D')    \n",
    "        usv_subset = ds_usv.sel(time=slice(orbit_time,orbit_time2))\n",
    "        for iusv in range(ilen):\n",
    "            if (ds_usv.time[iusv]<orbit_time) or (ds_usv.time[iusv]>orbit_time2):\n",
    "                continue\n",
    "            pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "            tree.query(pts,k=1)\n",
    "            i=tree.query(pts)[1]\n",
    "            ii=np.int(np.floor(i/1560))\n",
    "            jj=np.mod(i,1560)\n",
    "            deltaTa = ((ds_usv.time[iusv]-ds.time[ii,jj,ilook]).data)/ np.timedelta64(1,'m')\n",
    "            if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv].data/ np.timedelta64(1,'m')):\n",
    "                ds_usv.deltaT[iusv]=deltaTa\n",
    "                ds_usv.smap_SSS[iusv]=ds.sss_smap[ii,jj,ilook]\n",
    "                ds_usv.smap_name[iusv]=file2\n",
    "                ds_usv.smap_ydim[iusv]=ii\n",
    "                ds_usv.smap_xdim[iusv]=jj\n",
    "    fileout = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/rss40km'+str(ilook)+'_usv.nc'\n",
    "    ds_usv.to_netcdf(fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.DataArray 'deltaT' ()>\n",
      "array(5999940000000000, dtype='timedelta64[ns]')\n",
      "Coordinates:\n",
      "    trajectory  float32 1002.0\n",
      "    deltaT      timedelta64[ns] 69 days 10:39:00\n",
      "-1374.9180717166666\n",
      "-1374.9180717166666\n",
      "99999.0\n"
     ]
    }
   ],
   "source": [
    "print(ds_usv.deltaT[iusv])\n",
    "print(deltaTa)\n",
    "deltaTa = ((ds_usv.time[iusv]-ds.time[ii,jj,ilook]).data)/ np.timedelta64(1,'m')\n",
    "print(deltaTa)\n",
    "print(np.abs(ds_usv.deltaT[iusv].data/ np.timedelta64(1,'m')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1375 minutes\n"
     ]
    }
   ],
   "source": [
    "ds_usv['deltaT']=np.ones(ilen,dtype=np.timedelta64)*99999*6e10 #np.timedelta64(999999,'m')\n",
    "if np.abs(deltaT)<np.abs(ds_usv.deltaT[iusv].data):\n",
    "    print(deltaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1375 minutes 999999 nanoseconds\n"
     ]
    }
   ],
   "source": [
    "print(deltaT,ds_usv.deltaT[iusv].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv.sel(time=orbit_time, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds.cellat[:,:,0]-ds.cellat[:,:,1]).plot(vmin=-.1,vmax=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds2.cellat.data,ds2.cellon.data)\n",
    "print(ds_usv.lat[1000].data,ds_usv.lon[1000].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.query(pts))\n",
    "print(inputdata[tree.query(pts)[1]])\n",
    "print([ds_usv.lon[1000].data+360, ds_usv.lat[1000].data])\n",
    "print(len(inputdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_usv.lon[1000].data,ds_usv.lat[1000].data)\n",
    "print(jj,ii, ii*1560+jj)\n",
    "print(ds.cellon[ii,jj,0].data-360,ds.cellat[ii,jj,0].data)\n",
    "print(lons[ii,jj]-360,lats[ii,jj])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon[:,:,0].data\n",
    "y = ds.cellat[:,:,0].data\n",
    "z = ds.sss_smap[:,:,0].data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "ax.plot(ds.cellon[jj,ii,0],ds.cellat[jj,ii,0],'b*')\n",
    "ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmax(ds.cellon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "print(minlon,maxlon,minlat,maxlat)\n",
    "#cond = (lons>=minlon) & (lons<=maxlon) & (lats>=minlat) & (lats<=maxlat)\n",
    "#meets_condition = (air_day.air > 22) & (air_day.air < 30)\n",
    "#subset = ds.where((lons>=minlon) & (lons<=maxlon) & (lats>=minlat) & (lats<=maxlat),drop=True)\n",
    "subset = ds.where((lons>=minlon) & (lons<=maxlon) & (lats>=minlat) & (lats<=maxlat))\n",
    "print(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in ACCESS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'F:/data/cruise_data/access/rockfish_casts_2011.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds = ds.swap_dims({'row':'time'})\n",
    "ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds_access1 = ds.copy(deep=True)\n",
    "filename = 'F:/data/cruise_data/access/rockfish_casts_2015.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds = ds.swap_dims({'row':'time'})\n",
    "ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds_access2 = ds.copy(deep=True)\n",
    "ds_access = xr.concat((ds_access1,ds_access2),dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_access.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_access.time.dt.year,ds_access.time.dt.dayofyear,'.')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/access_date.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(ds_access.lon, ds_access.lat, s=3.0, c=ds_access.salinity, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,30,50\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/access_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in 2018 Saildrone West Coast Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://coastwatch.pfeg.noaa.gov/erddap/\n",
    "#url = 'https://coastwatch.pfeg.noaa.gov/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018'\n",
    "#url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv['lat']=ds_usv['s.latitude']\n",
    "ds_usv['lon']=ds_usv['s.longitude']\n",
    "ds_usv['time']=ds_usv['s.time']\n",
    "ds_usv['trajectory']=ds_usv['s.trajectory']\n",
    "ds_usv['SAL_MEAN']=ds_usv['s.SAL_MEAN']\n",
    "ds_usv['TEMP_CTD_MEAN']=ds_usv['s.TEMP_CTD_MEAN']\n",
    "ds_usv = ds_usv.swap_dims({'s':'time'})\n",
    "#print(ds_usv.time.min().data,ds_usv.time.max().data)\n",
    "#ds_usv.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this code to figure out where the data goes bad\n",
    "ilen = ds_usv.SAL_MEAN.size\n",
    "print(ilen)\n",
    "tem = np.nan(ilen)\n",
    "for i in range(787066):\n",
    "    if ds_usv.SAL_MEAN[i]>20:\n",
    "        tem[i]=ds_usv.SAL_MEAN[i]\n",
    "tem[i:]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is something bad in the data file above 776100 so subset the data to just the good part\n",
    "ds_usv2 = ds_usv.isel(time=slice(None,776100))\n",
    "xlon =  ds_usv2.lon.copy(deep=True)\n",
    "xlat =  ds_usv2.lat.copy(deep=True)\n",
    "salinity = ds_usv2.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -130,-114,30,52\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/wcoast_location2.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv2.trajectory[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018'\n",
    "#url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ferret.pmel.noaa.gov/generic/erddap/tabledap/saildrone_arctic_gts'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv['lat']=ds_usv['s.latitude']\n",
    "ds_usv['lon']=ds_usv['s.longitude']\n",
    "ds_usv['time']=ds_usv['s.time']\n",
    "ds_usv['trajectory']=ds_usv['s.trajectory']\n",
    "ds_usv['SAL_MEAN']=ds_usv['s.SAL_MEAN']\n",
    "ds_usv['TEMP_CTD_MEAN']=ds_usv['s.TEMP_CTD_MEAN']\n",
    "#ds_usv = ds_usv.swap_dims({'s':'time'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlon =  ds_usv.lon.copy(deep=True)\n",
    "xlat =  ds_usv.lat.copy(deep=True)\n",
    "salinity = ds_usv.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -130,-114,30,52\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/arctic_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "xlon =  ds_usv.longitude.copy(deep=True)\n",
    "xlat =  ds_usv.latitude.copy(deep=True)\n",
    "salinity =  ds_usv.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,25,40\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/wcoast_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filelist[8])\n",
    "ds.close()\n",
    "x = ds.cellon[:,:,0].data\n",
    "y = ds.cellat[:,:,0].data\n",
    "z = ds.sss_smap[:,:,0].data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "ax.coastlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create daily data arrays\n",
    "#grid the data and search within the in situ obs to find which filenames are in right region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create daily data arrays\n",
    "rlon=np.arange(-180,180,.25)\n",
    "rlat=np.arange(90,-90,-.25)\n",
    "area_def = load_area('areas.cfg', 'pc_world')\n",
    "itime=0\n",
    "for ilook in range(1):\n",
    "    x,y,x2,y2 = [],[],[],[]\n",
    "    if itime==1:\n",
    "        z = z2 = np.empty(0, np.int32)\n",
    "    else:\n",
    "        z = z2 = np.empty(0, np.datetime64)    \n",
    "    for file in filelist:\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds.close()\n",
    "        x = np.append(x,ds.cellon[:,:780,ilook].data)\n",
    "        y = np.append(y,ds.cellat[:,:780,ilook].data)\n",
    "        if itime==1:\n",
    "            z = np.append(z,ds.sss_smap[:,:780,ilook].data)\n",
    "        else:\n",
    "            z = np.append(z,ds.time[:,:780,ilook].data)\n",
    "        x2 = np.append(x2,ds.cellon[:,780:,ilook].data)\n",
    "        y2 = np.append(y2,ds.cellat[:,780:,ilook].data)\n",
    "        if itime==1:\n",
    "            z2 = np.append(z2,ds.sss_smap[:,780:,ilook].data)\n",
    "        else:\n",
    "            z2 = np.append(z2,ds.time[:,780:,ilook].data)\n",
    "    lons,lats,data = x,y,z \n",
    "    lons = np.mod(lons+180,360)-180\n",
    "    swath_def = SwathDefinition(lons, lats)\n",
    "    result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "    lons,lats,data = x2,y2,z2 \n",
    "    lons = np.mod(lons+180,360)-180\n",
    "    swath_def = SwathDefinition(lons, lats)\n",
    "    result2 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "    tem = np.stack((result1,result2))\n",
    "    das = xr.DataArray(tem,name='sss1',coords={'iasc':[0,1],'lat':rlat,'lon':rlon},dims=('iasc','lat','lon'))\n",
    "    dat = xr.DataArray(tem,name='time',coords={'iasc':[0,1],'lat':rlat,'lon':rlon},dims=('iasc','lat','lon'))\n",
    "ds_out = xr.Dataset({das,dat})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = xr.Dataset({das,dat})\n",
    "z=np.empty(0,np.datetime64)\n",
    "print(type(z),type(ds.time[0,0,0].data))\n",
    "z = np.append(z,ds.time[0,0,0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    da0 = xr.DataArray(result,name='sss0',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "\n",
    "#iasc=0\n",
    "#da1 = xr.DataArray(result,name='sss1',coords={'lat':rlat,'lon':rlon,'asc':iasc},dims=('lat','lon','asc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da1.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = x #ds.cellon[:,:,0].data\n",
    "lons = np.mod(lons+180,360)-180\n",
    "lats = y# ds.cellat[:,:,0].data\n",
    "data = z#ds.sss_smap[:,:,0].data\n",
    "area_def = load_area('areas.cfg', 'pc_world')\n",
    "swath_def = SwathDefinition(lons, lats)\n",
    "result = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "save_quicklook('sss.png', area_def, result, num_meridians=0, num_parallels=0, label='Salinity (psu)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rlon=np.arange(-180,180,.25)\n",
    "rlat=np.arange(90,-90,-.25)\n",
    "da = xr.DataArray(result,name='sss0',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contourf(rlon,rlat,result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interpolate onto grid\n",
    "# data coordinates and values\n",
    "idim = ds.cellon.shape[0]\n",
    "jdim = ds.cellon.shape[1]\n",
    "x = ds.cellon[:,:,0].data.reshape(idim*jdim)\n",
    "y = ds.cellat[:,:,0].data.reshape(idim*jdim)\n",
    "z = ds.sss_smap[:,:,0].data.reshape(idim*jdim)\n",
    "\n",
    "mask = np.isfinite(x) & np.isfinite(y)\n",
    "x = x[mask]\n",
    "y = y[mask]\n",
    "z = z[mask]\n",
    "\n",
    "# target grid to interpolate to\n",
    "xi,yi = np.arange(0,360,.5), np.arange(-90,90,.5)\n",
    "xi,yi = np.meshgrid(xi,yi)\n",
    "\n",
    "## set mask\n",
    "#mask = (xi > 0.5) & (xi < 0.6) & (yi > 0.5) & (yi < 0.6)\n",
    "\n",
    "# interpolate\n",
    "zi = griddata((x,y),z,(xi,yi),method='nearest')\n",
    "\n",
    "plt.contourf(xi,yi,zi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use open_mfdataset you need to either provide a path or a list of filenames to input\n",
    "\n",
    "Here we use the USV cruise start and end date to read in all data for that period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in SSS L2 data\n",
    "Read in data using open_mfdataset with the option coords='minimal'\n",
    "\n",
    "The dataset is printed out and you can see that rather than straight xarray data array for each of the data variables open_mfdataset using dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = np.arange(0,360)\n",
    "np.mod(lon+180,360)-180\n",
    "np.mod(lon + 180,360) - 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'f:/data/sat_data/aquarius/Q2011237.L3m_DAY_SCIA_V5.0.RAIN_MASK_SSS_1deg'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds.close()\n",
    "ds.coords['phony_dim_0']=np.arange(90,-90,-1)\n",
    "ds.coords['phony_dim_1']=np.arange(-180,180)\n",
    "ds = ds.rename({'phony_dim_0':'lat','phony_dim_1':'lon'})\n",
    "ds.l3m_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ds_sat = xr.open_mfdataset(filelist,coords='minimal')\n",
    "ds_sat = xr.open_dataset(filelist[0])\n",
    "#ds_sat = ds_sat.isel(depth=0).drop('year').rename({'latitude':'lat'}).rename({'longitude':'lon'})\n",
    "#ds_sat = ds_sat.sortby('lat').sel(lon=slice(20.0,379.00))\n",
    "#ds_sat.coords['lon'] = (ds_sat.coords['lon'] + 180) % 360 - 180\n",
    "#ds_sat = ds_sat.sortby('lon')\n",
    "ds_sat.close()\n",
    "print(ds_sat)  #check units \n",
    "ds_sat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. First let's subset the data to make it smaller to deal with by using the cruise lat/lons\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1 from above\n",
    "subset = ds_sat.sel(lon=slice(ds_usv_subset.lon.min().data,ds_usv_subset.lon.max().data),\n",
    "                    lat=slice(ds_usv_subset.lat.min().data,ds_usv_subset.lat.max().data))\n",
    "\n",
    "#now collocate with usv lat and lons\n",
    "ds_collocated = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='linear')\n",
    "ds_collocated_nearest = subset.interp(lat=ds_usv_subset.lat,lon=ds_usv_subset.lon,time=ds_usv_subset.time,method='nearest')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A larger STD that isn't reflective of uncertainty in the observation\n",
    "The collocation above will result in multiple USV data points matched with a single satellite\n",
    "observation.    The USV is sampling every 1 min and approximately few meters, while the satellite\n",
    "is an average over a footprint that is interpolated onto a daily mean map.  While calculating the mean would results in a valid mean, the STD would be higher and consist of a component that reflects the uncertainty of the USV and the satellite and a component that reflects the natural variability in the region that is sampled by the USV\n",
    "\n",
    "Below we use the 'nearest' collocation results to identify when multiple USV data are collcated to\n",
    "a single satellite observation.\n",
    "This code goes through the data and creates averages of the USV data that match the single CCMP collocated value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ilen,index = ds_collocated_nearest.dims['time'],0\n",
    "ds_tem = ds_collocated_nearest.copy(deep=True)\n",
    "duu, duv1, duv2, dlat, dlon, dut = [],[],[],[],[],np.empty((),dtype='datetime64')\n",
    "while index <= ilen-2:\n",
    "    index += 1\n",
    "    if np.isnan(ds_collocated_nearest.u[index]):\n",
    "        continue\n",
    "    if np.isnan(ds_tem.u[index]):\n",
    "        continue\n",
    "   # print(index, ilen)\n",
    "    iend = index + 730\n",
    "    if iend > ilen-1:\n",
    "        iend = ilen-1\n",
    "    ds_tem_subset = ds_tem.u[index:iend]\n",
    "    ds_tem_subset2 = ds_tem.v[index:iend]\n",
    "    ds_usv_subset2ucur = ds_usv_subset.vel_east_30m[index:iend]\n",
    "    ds_usv_subset2vcur = ds_usv_subset.vel_north_30m[index:iend]\n",
    "    ds_usv_subset2lat = ds_usv_subset.lat[index:iend]\n",
    "    ds_usv_subset2lon = ds_usv_subset.lon[index:iend]\n",
    "    ds_usv_subset2time = ds_usv_subset.time[index:iend]\n",
    "    cond = ((ds_tem_subset==ds_collocated_nearest.u[index]) & (ds_tem_subset2==ds_collocated_nearest.v[index]))\n",
    "    notcond = np.logical_not(cond)\n",
    "    #cond = ((ds_tem.analysed_sst==ds_collocated_nearest.analysed_sst[index]))\n",
    "    #notcond = np.logical_not(cond)\n",
    "    masked = ds_tem_subset.where(cond)\n",
    "    if masked.sum().data==0:  #don't do if data not found\n",
    "        continue\n",
    "    masked_usvucur = ds_usv_subset2ucur.where(cond,drop=True)\n",
    "    masked_usvvcur = ds_usv_subset2vcur.where(cond,drop=True)\n",
    "    masked_usvlat = ds_usv_subset2lat.where(cond,drop=True)\n",
    "    masked_usvlon = ds_usv_subset2lon.where(cond,drop=True)\n",
    "    masked_usvtime = ds_usv_subset2time.where(cond,drop=True)\n",
    "    duu=np.append(duu,masked_usvucur.mean().data)\n",
    "    duv1=np.append(duv1,masked_usvvcur.mean().data)\n",
    "    dlat=np.append(dlat,masked_usvlat.mean().data)\n",
    "    dlon=np.append(dlon,masked_usvlon.mean().data)\n",
    "    tdif = masked_usvtime[-1].data-masked_usvtime[0].data\n",
    "    mtime=masked_usvtime[0].data+np.timedelta64(tdif/2,'ns')\n",
    "#    if mtime>dut.max():\n",
    "#        print(index,dut.shape[0],masked_usvtime[0].data,masked_usvtime[-1].data-masked_usvtime[0].data)\n",
    "    dut=np.append(dut,mtime)\n",
    "    ds_tem.u[index:iend]=ds_tem.u.where(notcond)\n",
    "    ds_tem.v[index:iend]=ds_tem.v.where(notcond)\n",
    "dut2 = dut[1:]  #remove first data point which is a repeat from what array defined       \n",
    "ds_new=xr.Dataset(data_vars={'vel_east': ('time',duu),'vel_north':('time',duv1),\n",
    "                             'lon': ('time',dlon),\n",
    "                             'lat': ('time',dlat)},\n",
    "                  coords={'time':dut2})\n",
    "ds_new.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/oscar_downsampled_usv_data2.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# redo the collocation\n",
    "Now, redo the collocation, using 'linear' interpolation using the averaged data.  This will interpolate the data temporally onto the USV sampling which has been averaged to the satellite data grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged = subset.interp(lat=ds_new.lat,lon=ds_new.lon,time=ds_new.time,method='linear')\n",
    "ds_collocated_averaged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged.to_netcdf('F:/data/cruise_data/saildrone/baja-2018/oscar_downsampled_collocated_usv_data3.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_collocated_averaged['spd']=np.sqrt(ds_collocated_averaged.u**2+ds_collocated_averaged.v**2)\n",
    "ds_new['spd'] = np.sqrt(ds_new.vel_east**2+ds_new.vel_north**2)\n",
    "ds_collocated_averaged['dir']=np.arctan2(ds_collocated_averaged.v,ds_collocated_averaged.u)*180./np.pi\n",
    "ds_new['dir'] = np.arctan2(ds_new.vel_north,ds_new.vel_east)*180./np.pi\n",
    "\n",
    "usv_spd = ds_new.spd\n",
    "sat_spd = ds_collocated_averaged.spd\n",
    "usv_dir = ds_new.dir\n",
    "sat_dir = ds_collocated_averaged.dir\n",
    "dif_spd,dif_dir = usv_spd - sat_spd, usv_dir - sat_dir\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "dif_dir[cond]-=360\n",
    "dif_dir[cond2]+=360\n",
    "print('mean,std dif speed',[dif_spd.mean().data,dif_spd.std().data])\n",
    "print('mean,std dir',[dif_dir.mean().data,dif_dir.std().data,dif_spd.shape[0]])\n",
    "\n",
    "usv_spd = ds_new.spd\n",
    "sat_spd = ds_collocated_averaged.spd\n",
    "usv_dir = ds_new.dir\n",
    "sat_dir = ds_collocated_averaged.dir\n",
    "dif_spd,dif_dir = usv_spd - sat_spd, usv_dir - sat_dir\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "dif_dir[cond]-=360\n",
    "dif_dir[cond2]+=360\n",
    "\n",
    "cond = (np.isfinite(usv_spd) & np.isfinite(sat_spd))\n",
    "usv_spd = usv_spd[cond]\n",
    "sat_spd = sat_spd[cond]\n",
    "usv_dir = usv_dir[cond]\n",
    "sat_dir = sat_dir[cond]\n",
    "\n",
    "sdif = dif_spd.dropna('time')\n",
    "sdifcor = np.corrcoef(sat_spd,usv_spd)[0,1]\n",
    "std_robust = np.nanmedian(np.abs(sdif - np.nanmedian(sdif))) * 1.482602218505602\n",
    "ilen = sdif.shape[0]\n",
    "print([sdif.mean().data,sdif.median().data,sdifcor,sdif.std().data,std_robust,\n",
    "                    np.abs(sdif).mean().data,sdif.shape[0]])\n",
    "sdif = dif_dir.dropna('time')\n",
    "sdifcor = np.corrcoef(sat_dir,usv_dir)[0,1]\n",
    "std_robust = np.nanmedian(np.abs(sdif - np.nanmedian(sdif))) * 1.482602218505602\n",
    "ilen = sdif.shape[0]\n",
    "print([sdif.mean().data,sdif.median().data,sdifcor,sdif.std().data,std_robust,\n",
    "                    np.abs(sdif).mean().data,sdif.shape[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# Find your api_key here: https://plot.ly/settings/api\n",
    "# MatPlotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "# Scientific libraries\n",
    "from numpy import arange,array,ones\n",
    "from scipy import stats\n",
    "\n",
    "usv_spd = ds_new.spd\n",
    "sat_spd = ds_collocated_averaged.spd\n",
    "usv_dir = ds_new.dir\n",
    "sat_dir = ds_collocated_averaged.dir\n",
    "dif_spd,dif_dir = usv_spd - sat_spd, usv_dir - sat_dir\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "cond,cond2 = (dif_dir > 180),(dif_dir < -180)\n",
    "dif_dir[cond]-=360\n",
    "dif_dir[cond2]+=360\n",
    "\n",
    "usv_ucur = ds_new.vel_east\n",
    "usv_vcur = ds_new.vel_north\n",
    "sat_ucur = ds_collocated_averaged.u\n",
    "sat_vcur = ds_collocated_averaged.v\n",
    "usv_spd  = np.sqrt(usv_ucur**2 + usv_vcur**2)\n",
    "sat_spd  = np.sqrt(sat_ucur**2 + sat_vcur**2)\n",
    "\n",
    "cond = (np.isfinite(usv_spd) & np.isfinite(sat_spd))\n",
    "usv_spd = usv_spd[cond]\n",
    "sat_spd = sat_spd[cond]\n",
    "usv_dir = usv_dir[cond]\n",
    "sat_dir = sat_dir[cond]\n",
    "\n",
    "xi = usv_spd.data\n",
    "A = array([ xi, ones(sat_spd.shape[0])])\n",
    "y = sat_spd.data\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "plt.plot(xi,y,'o', xi, line)\n",
    "plt.ylim(-.1,1),plt.xlim(-.1,1)\n",
    "plt.grid()\n",
    "plt.xlabel('USV speed')\n",
    "plt.ylabel('OSCAR speed')\n",
    "print(slope,intercept,r_value,p_value,std_err)\n",
    "\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/oscar_usv_big_spd.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n",
    "\n",
    "#pylab.title('Linear Fit with Matplotlib')\n",
    "#ax = plt.gca()\n",
    "#ax.set_axis_bgcolor((0.898, 0.898, 0.898))\n",
    "#fig = plt.gcf()\n",
    "#py.plot_mpl(fig, filename='linear-Fit-with-matplotlib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usv_ucur = ds_new.vel_east\n",
    "usv_vcur = ds_new.vel_north\n",
    "sat_ucur = ds_collocated_averaged.u\n",
    "sat_vcur = ds_collocated_averaged.v\n",
    "\n",
    "cond = (np.isfinite(usv_ucur) & np.isfinite(sat_ucur) & ( usv_vcur<.2))\n",
    "usv_ucur = usv_ucur[cond]\n",
    "sat_ucur = sat_ucur[cond]\n",
    "usv_vcur = usv_vcur[cond]\n",
    "sat_vcur = sat_vcur[cond]\n",
    "\n",
    "xi = usv_ucur.data\n",
    "A = array([ xi, ones(usv_ucur.shape[0])])\n",
    "y = sat_ucur.data\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "plt.subplot(231)\n",
    "plt.plot(xi,y,'o', xi, line)\n",
    "plt.ylim(-.5,.5),plt.xlim(-.5,.5)\n",
    "plt.grid()\n",
    "plt.xlabel('USV east speed')\n",
    "plt.ylabel('SAT east speed')\n",
    "print(slope,intercept,r_value,p_value,std_err)\n",
    "xi = usv_vcur.data\n",
    "A = array([ xi, ones(usv_vcur.shape[0])])\n",
    "y = sat_vcur.data\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "plt.subplot(232)\n",
    "plt.plot(xi,y,'o', xi, line)\n",
    "plt.xlabel('USV north speed')\n",
    "plt.ylabel('SAT north speed')\n",
    "print(slope,intercept,r_value,p_value,std_err)\n",
    "plt.ylim(-.5,.5),plt.xlim(-.5,.5)\n",
    "plt.grid()\n",
    "xi = np.sqrt(usv_vcur.data**2 + usv_ucur.data**2)\n",
    "A = array([ xi, ones(usv_vcur.shape[0])])\n",
    "y = np.sqrt(sat_vcur.data**2+sat_ucur.data**2)\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "plt.subplot(233)\n",
    "plt.plot(xi,y,'o', xi, line)\n",
    "plt.ylim(-.1,.75),plt.xlim(-.1,.75)\n",
    "plt.grid()\n",
    "plt.xlabel('USV speed')\n",
    "plt.ylabel('SAT speed')\n",
    "print(slope,intercept,r_value,p_value,std_err)\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/sat_current_U_V_speed.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(usv_spd,usv_spd - sat_spd,'.')\n",
    "ax.grid(True)\n",
    "ax.set_xlabel('USV current speed (ms$^{-1}$)')\n",
    "ax.set_ylabel('USV - Sat current speed (ms$^{-1}$)')\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/sat_current_USV_minus_Sat_fnct_USV.png'\n",
    "fig.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5,4))\n",
    "ax.plot(usv_spd,usv_dir - sat_dir,'.')\n",
    "ax.set_xlabel('USV current speed (ms$^{-1}$)')\n",
    "ax.set_ylabel('USV - Sat current direction (deg)')\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/sat_current_both_bias.png'\n",
    "fig.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv_subset['cur_spd']=np.sqrt(ds_usv_subset.vel_east**2+ds_usv_subset.vel_north**2)\n",
    "ds_usv_6hr=ds_usv_subset.resample(time='6H').mean()\n",
    "ds_usv_6hr['cur_spd']=np.sqrt(ds_usv_6hr.vel_east**2+ds_usv_6hr.vel_north**2)\n",
    "\n",
    "\n",
    "plt.plot(ds_collocated_averaged.time[cond],sat_spd,'.-')\n",
    "plt.plot(ds_collocated_averaged.time[cond],usv_spd,'.-')\n",
    "plt.plot(ds_usv_6hr.time,ds_usv_6hr.cur_spd)\n",
    "plt.legend({'OSCAR','USV','USV-ave'})\n",
    "fig_fname='F:/data/cruise_data/saildrone/baja-2018/figs/sat_current_timeseries_bias.png'\n",
    "fig.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv.spd_30m.sel(time=slice('2018-05-05','2018-05-09')).plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
