{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample.geometry import GridDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from scipy import spatial\n",
    "sys.path.append('../saildrone/subroutines/')\n",
    "from read_routines import read_all_usv, read_one_usv, add_coll_vars,get_filelist_l2p,get_orbital_data_l2p\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/' #'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/*.nc' \n",
    "#dir_data 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "#dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "dir_out = 'F:/data/cruise_data/saildrone/sss_collocations_8day/'\n",
    "\n",
    "data_dict = read_all_usv(dir_data_pattern)\n",
    "data_dict = add_coll_vars(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSS test open\n",
    "#file = 'F:/data/sat_data/smap/SSS/L3/RSS/V4/8day_running/SCI/2016/001/RSS_smap_SSS_L3_8day_running_2016_005_FNL_v04.0.nc'\n",
    "#ds = xr.open_dataset(file)\n",
    "#ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "#ds = ds.sortby(ds.lon)\n",
    "#ds.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RSS\n",
    "#get list of all filenames in directory\n",
    "adir = 'F:/data/sat_data/smap/SSS/L3/RSS/V4/8day_running/SCI/**/**/*.nc'\n",
    "files = [x for x in glob(adir)]\n",
    "print('number of file:',len(files))\n",
    "\n",
    "ds = xr.open_mfdataset(files,combine='nested',concat_dim='time')\n",
    "ds.coords['lon'] = (ds.coords['lon'] + 180) % 360 - 180\n",
    "ds = ds.sortby(ds.lon)\n",
    "ds.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iname,name in enumerate(data_dict):\n",
    "    if not iname==0:\n",
    "        continue\n",
    "    ds_usv = data_dict[name]\n",
    "    ds_usv['lat'] = ds_usv.lat.interpolate_na(dim='time',method='linear')\n",
    "    ds_usv['lon'] = ds_usv.lon.interpolate_na(dim='time',method='linear')\n",
    "    t1,t2=ds_usv.time.min().data-np.timedelta64(8,'D'),ds_usv.time.max().data+np.timedelta64(8,'D')\n",
    "    x1,x2=ds_usv.lon.min().data,ds_usv.lon.max().data\n",
    "    y1,y2=ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "    print(t1,t2)\n",
    "    ds_sat = ds.sel(time=slice(t1,t2),lat=slice(y1,y2),lon=slice(x1,x2)).load()   \n",
    "    ds_interp = ds_sat.interp(time=ds_usv.time,lat=ds_usv.lat,lon=ds_usv.lon,method='nearest')\n",
    "    fout = dir_out+name+'_RSS8dy'+'.nc'\n",
    "    ds_interp.to_netcdf(fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JPL\n",
    "file = 'F:/data/sat_data/smap/SSS/L3/JPL/V4.3/8day_running/2015/125/SMAP_L3_SSS_20150509_8DAYS_V4.3.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds.close()  \n",
    "ds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
