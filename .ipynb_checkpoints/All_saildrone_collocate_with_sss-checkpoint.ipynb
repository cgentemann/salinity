{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from scipy import spatial\n",
    "sys.path.append('./subroutines/')\n",
    "from read_routines import read_all_usv, get_filelist_l2p,get_orbital_data_l2p\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of file: 44\n",
      "0 aildrone\\PMEL_Arctic_2015_sd126-ALL-1_min-v1\n",
      "1 aildrone\\PMEL_Arctic_2015_sd128-ALL-1_min-v1\n",
      "2 aildrone\\PMEL_Arctic_2016_sd126-ALL-1_min-v1\n",
      "3 aildrone\\PMEL_Arctic_2016_sd128-ALL-1_min-v1\n",
      "4 aildrone\\saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1\n",
      "5 aildrone\\saildrone-gen_4-shark-2018-sd1001-20180315T000000-20180529T235959-1_minutes-v1.1581626958976\n",
      "6 aildrone\\saildrone-gen_4-shark-2018-sd1004-20180315T000000-20180617T235959-1_minutes-v1.1581627077777\n",
      "7 aildrone\\saildrone-gen_5-1021_atlantic-sd1021-20190525T000000-20191021T235959-1_minutes-v1.1571806429446_(1)\n",
      "8 aildrone\\saildrone-gen_5-antarctica_circumnavigation_2019-sd1020-20190119T040000-20190803T043000-1_minutes-v1.1564884498845\n",
      "9 aildrone\\saildrone-gen_5-atomic_eurec4a_2020-sd1026-20200117T000000-20200302T235959-1_minutes-v1.1589306725934\n",
      "10 aildrone\\saildrone-gen_5-atomic_eurec4a_2020-sd1060-20200117T000000-20200302T235959-1_minutes-v1.1589306886594\n",
      "11 aildrone\\saildrone-gen_5-atomic_eurec4a_2020-sd1061-20200117T000000-20200302T235959-1_minutes-v1.1589307121602\n",
      "12 aildrone\\saildrone_arctic_2017_sd1001\n",
      "13 aildrone\\saildrone_arctic_2017_sd1002\n",
      "14 aildrone\\saildrone_arctic_2017_sd1003\n",
      "15 aildrone\\saildrone_arctic_sd1033_2019\n",
      "16 aildrone\\saildrone_arctic_sd1034_2019\n",
      "17 aildrone\\saildrone_arctic_sd1035_2019\n",
      "18 aildrone\\saildrone_arctic_sd1036_2019\n",
      "19 aildrone\\saildrone_arctic_sd1037_2019\n",
      "20 aildrone\\saildrone_arctic_sd1041_2019\n",
      "21 aildrone\\saildrone_tpos_sd1005_2017\n",
      "22 aildrone\\saildrone_tpos_sd1005_2018\n",
      "23 aildrone\\saildrone_tpos_sd1006_2017\n",
      "24 aildrone\\saildrone_tpos_sd1006_2018\n",
      "25 aildrone\\saildrone_tpos_sd1029_2018\n",
      "26 aildrone\\saildrone_tpos_sd1030_2018\n",
      "27 aildrone\\saildrone_tpos_sd1066_2019\n",
      "28 aildrone\\saildrone_tpos_sd1067_2019\n",
      "29 aildrone\\saildrone_tpos_sd1068_2019\n",
      "30 aildrone\\saildrone_tpos_sd1069_2019\n",
      "31 aildrone\\saildrone_west_coast_survey_2018_sd1024\n",
      "32 aildrone\\saildrone_west_coast_survey_2018_sd1025\n",
      "33 aildrone\\saildrone_west_coast_survey_2018_sd1026\n",
      "34 aildrone\\saildrone_west_coast_survey_2018_sd1027\n",
      "35 aildrone\\saildrone_west_coast_survey_2018_sd1028\n",
      "36 aildrone\\saildrone_west_coast_survey_2019_sd1038\n",
      "37 aildrone\\saildrone_west_coast_survey_2019_sd1039\n",
      "38 aildrone\\saildrone_west_coast_survey_2019_sd1040\n",
      "39 aildrone\\saildrone_west_coast_survey_2019_sd1043\n",
      "40 aildrone\\saildrone_west_coast_survey_2019_sd1044\n",
      "41 aildrone\\saildrone_west_coast_survey_2019_sd1045\n",
      "42 aildrone\\saildrone_west_coast_survey_2019_sd1046\n",
      "43 aildrone\\saildrone_west_coast_survey_2019_sd1047\n"
     ]
    }
   ],
   "source": [
    "dir_data = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/' #'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/*.nc' \n",
    "#dir_data 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "#dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "data_dict = read_all_usv(dir_data_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example showing how the using matplotlib maps orbital data quickly and easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_tem = xr.open_dataset('C:/Users/gentemann/Google Drive/public/2019_saildrone/saildrone_arctic_sd1037_2019.nc')\n",
    "#ds_tem2 = xr.open_dataset('f:/data/cruise_data/saildrone/2019_arctic/post_mission/saildrone-gen_5-arctic_misst_2019-sd1037-20190514T230000-20191011T183000-1_minutes-v1.1575487464625.nc')\n",
    "#ds_tem = ds_tem.isel(row=slice(60,-1))\n",
    "#ds_tem2 = ds_tem2.isel(obs=slice(60*24,-1))\n",
    "#print(ds_tem.time[0].data,ds_tem.time[-1].data)\n",
    "#print(ds_tem2.time[0,0].data,ds_tem2.time[0,-1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'saildrone_west_coast_survey_2019_sd1047'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-428435ea0cd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0madir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mds_usv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'saildrone_west_coast_survey_2019_sd1047'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'saildrone_west_coast_survey_2019_sd1047'"
     ]
    }
   ],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv = data_dict['saildrone_west_coast_survey_2019_sd1047']\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds.cellon[jj,ii],ds.cellat[jj,ii,0],'b*')\n",
    "ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What lon range for satellite & insitu? are we going 0-360 or -180 to 180?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cellon.min().data,ds.cellon.max().data)\n",
    "print(ds_usv.lon.min().data,ds_usv.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's figure out what orbital files actually have data in our area of interest.  To do this, use the pyresample software\n",
    "\n",
    "- read in the in situ data\n",
    "- calculate the in situ min/max dates to know what files to check\n",
    "\n",
    "Now we have our time of interest\n",
    "\n",
    "- loop through the satellite data\n",
    "- calculate the in situ min/max lat/lon on the same day to define a small box of interest\n",
    "- use pyresample to map the data onto a predefined 0.1 deg resolution spatial grid\n",
    "- subset the gridded map to the area of interest\n",
    "- see if there is any valid data in that area\n",
    "- if there is any valid data, go to next step\n",
    "\n",
    "## Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aildrone\\PMEL_Arctic_2015_sd126-ALL-1_min-v1\n",
      "2015-05-12T20:00:16.000000000 2015-07-28T19:58:16.000000000\n"
     ]
    }
   ],
   "source": [
    "#effort to combine the finding & collocating code\n",
    "#intialize grid\n",
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "#for name in data_dict:\n",
    "for iname,name in enumerate(data_dict):\n",
    "#    if iname>0:\n",
    "#        continue\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,1):\n",
    "\n",
    "        ds_usv,name_usv=data_dict[name],name\n",
    "\n",
    "        if isat==0:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave3.nc'\n",
    "        if isat==1:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave3.nc'   \n",
    "\n",
    "#        if path.exists(fileout):\n",
    "#            continue\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(iname,name)\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            filelist = get_filelist_l2p(isat, usv_day)\n",
    "            x,y,z = [],[],[]\n",
    "            for ifile,file in enumerate(filelist):\n",
    "#                if ifile!=7:\n",
    "#                    continue\n",
    "#            for file in filelist:\n",
    "                ds = xr.open_dataset(file)\n",
    "                ds.close()  \n",
    "                if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.isel(look=0)\n",
    "                    ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "                    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "                if isat==1:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.rename({'row_time':'time'})\n",
    "\n",
    "#first do a quick check using resample to project the orbit onto a grid \n",
    "#and quickly see if there is any data in the cruise area on that day\n",
    "#if there is, then continue to collocation\n",
    "\n",
    "                x = ds['lon'].fillna(-89).data \n",
    "                y = ds['lat'].fillna(-89).data \n",
    "                z = ds['smap_sss'].data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "                num_obs = np.isfinite(subset).sum()\n",
    "                if num_obs<1:  #no collocations so go to next orbit\n",
    "                    continue\n",
    "\n",
    "                #stack xarray dataset then drop lon == nan\n",
    "                ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "                #drop nan\n",
    "                ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "                lats = ds_drop.lat.data\n",
    "                lons = ds_drop.lon.data\n",
    "                inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "                tree = spatial.KDTree(inputdata)\n",
    "#                ilen = ds_usv.time.size\n",
    "                #find indices for ds_usv that are within 12 hours of orbit max/min time\n",
    "                orbit_time = ds.time.max().data-np.timedelta64(12,'h')   #CHANGED TO +-12 HR\n",
    "                orbit_time2 = ds.time.max().data+np.timedelta64(12,'h')    \n",
    "                cond = (ds_usv.time.data>orbit_time) & (ds_usv.time.data<orbit_time2)\n",
    "                item = np.argwhere(cond)\n",
    "                for iusv_index in range(int(item[0]),int(item[-1])):\n",
    "#                    if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "#                        continue\n",
    "                    pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "            #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "                    tree.query(pts,k=1)\n",
    "                    i = tree.query(pts)[1]\n",
    "                    rdist = tree.query(pts)[0]\n",
    "                    #don't use matchups more than 25 km away\n",
    "                    if rdist>.25:\n",
    "                        continue\n",
    "                    #use .where to find the original indices of the matched data point\n",
    "                    #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "                    result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "                    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "                    if len(listOfCoordinates)==0:\n",
    "                        continue\n",
    "                    ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "                    if isat==0:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "                    if isat==1:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "                    if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                        ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                        ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                        ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                        ds_usv.smap_name[iusv_index]=file\n",
    "                        ds_usv.smap_dist[iusv_index]=rdist\n",
    "                        ds_usv.smap_ydim[iusv_index]=ii\n",
    "                        ds_usv.smap_xdim[iusv_index]=jj\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        ds_usv.to_netcdf(fileout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop where no collocations in data\n",
    "ds_tem = ds_usv.where(ds_usv.smap_dist<25,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look for repeat obs\n",
    "#remove any duplicates\n",
    "_, index = np.unique(ds_tem.smap_name, return_index=True)\n",
    "#ds=ds.isel(time=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find values with same filename, scan number, and cell position\n",
    "#this takes the mean, but it may mix up measurements from 'good' obs with 'bad' obs\n",
    "#should either only use lowest iqc or ?\n",
    "ds_tem2 = ds_tem#.drop('traj')\n",
    "ds_new = ds_tem\n",
    "ds_mn = []\n",
    "#ds_tem2.drop({'obs','traj'})\n",
    "#for i in range(4): #len(ds_tem)):\n",
    "while len(ds_new)>1:\n",
    "    cond = ((ds_tem2.smap_name==ds_tem2.smap_name[i]) \n",
    "            & (ds_tem2.smap_ydim==ds_tem2.smap_ydim[i]) \n",
    "            & (ds_tem2.smap_xdim==ds_tem2.smap_xdim[i]))\n",
    "    subset = ds_tem2.where(cond,drop=True)  #repeat obs\n",
    "    ds_mn = subset.mean(keep_attrs=True,skipna=True)\n",
    "    ds_mn['time'] = subset.time.mean()\n",
    "    ds_mn = ds_mn.assign_coords({'ob':i})\n",
    "    if i==0:\n",
    "        ds_mn2 = ds_mn\n",
    "    else:\n",
    "        ds_mn2 = xr.concat([ds_mn2,ds_mn],dim='ob')\n",
    "    ds_new = ds_tem2.where(~cond,drop=True)  #data with repeat obs removed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.smap_iqc_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv.smap_iqc_flag[10000:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.quality_flag[200:400,330:400].plot(vmin=0,vmax=2192)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.smap_sss[200:400,330:400].plot()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(300,350,5):\n",
    "    print(ds.smap_sss[i,370].data,ds.quality_flag[i,370].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = ds.quality_flag.attrs['flag_meanings'].split(' ')\n",
    "for i in range(16):\n",
    "    print(i,qc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0     1      2       3     4       5     6     7       8      9      10     11      12       13\n",
    "0*2**0+0*2**1+0*2**2+0*2**3+0*2**4+0*2**5+0*2**6+0*2**7+0*2**8+0*2**9+1*2**10+0*2**11+0*2**12+0*2**13\n",
    "#bytearray(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.smap_sss[320,360:370].data)\n",
    "print(ds.quality_flag[320,360:370].data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifile,file in enumerate(filelist):\n",
    "    if ifile==7:\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds = ds.isel(look=0)\n",
    "        ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "        ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "        plt.scatter(ds.lon,ds.lat)\n",
    "        plt.scatter(ds_day.lon,ds_day.lat)\n",
    "        plt.title(ifile)\n",
    "        plt.xlim(-140,-100)\n",
    "        plt.ylim(0,50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = np.argwhere(cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_usv.time[i[-1]].data,orbit_time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "filelist = get_filelist_l2p(isat, usv_day)\n",
    "ds = xr.open_mfdataset(filelist,combine='nested',concat_dim='phony_dim_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iusv_index in range(ilen):\n",
    "    if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "        continue\n",
    "    pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "#        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "    tree.query(pts,k=1)\n",
    "    i = tree.query(pts)[1]\n",
    "    rdist = tree.query(pts)[0]\n",
    "    #don't use matchups more than 25 km away\n",
    "    if rdist>.25:\n",
    "        continue\n",
    "    #use .where to find the original indices of the matched data point\n",
    "    #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "    result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "    if len(listOfCoordinates)==0:\n",
    "        continue\n",
    "    ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "    if isat==0:\n",
    "        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "    if isat==1:\n",
    "        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "    if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "        ds_usv.deltaT[iusv_index]=deltaTa\n",
    "        ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "        ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "        ds_usv.smap_name[iusv_index]=file\n",
    "        ds_usv.smap_dist[iusv_index]=rdist\n",
    "        ds_usv.smap_ydim[iusv_index]=ii\n",
    "        ds_usv.smap_xdim[iusv_index]=jj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_usv.time[1800].data,orbit_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                orbit_time = ds.time.max().data-np.timedelta64(1,'D')\n",
    "                orbit_time2 = ds.time.max().data+np.timedelta64(1,'D')    \n",
    "                ds_tem = ds_usv.sel(time=slice(orbit_time,orbit_time2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.quality_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
