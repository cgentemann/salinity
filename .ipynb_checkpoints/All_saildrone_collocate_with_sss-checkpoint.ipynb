{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample.geometry import GridDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from scipy import spatial\n",
    "sys.path.append('../saildrone/subroutines/')\n",
    "from read_routines import read_all_usv, add_coll_vars,get_filelist_l2p,get_orbital_data_l2p\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/' #'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/*.nc' \n",
    "#dir_data 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "#dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "data_dict = read_all_usv(dir_data_pattern)\n",
    "data_dict = add_coll_vars(data_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example showing how the using matplotlib maps orbital data quickly and easily\n",
    "def add_coll_vars(data_dict):\n",
    "    import xarray as xr\n",
    "    import numpy as np\n",
    "    for name in data_dict:\n",
    "        ds = data_dict[name]\n",
    "        # add room to write collocated data information\n",
    "        ilen = ds.time.shape[0]\n",
    "        ds['deltaT'] = xr.DataArray(np.ones(ilen, dtype='float32')*99999, coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_SSS'] = xr.DataArray(np.empty(ilen, dtype='float32'), coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_iqc_flag'] = xr.DataArray(np.empty(ilen, dtype='int32'), coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_name'] = xr.DataArray(np.empty(ilen, dtype='U125'), coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_dist'] = xr.DataArray(np.ones(ilen, dtype='float32')*99999, coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_ydim'] = xr.DataArray(np.empty(ilen, dtype='float32'), coords={'time': ds.time}, dims=('time'))\n",
    "        ds['smap_xdim'] = xr.DataArray(np.empty(ilen, dtype='float32'), coords={'time': ds.time}, dims=('time'))\n",
    "        data_dict[name]=ds\n",
    "    return data_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_tem = xr.open_dataset('C:/Users/gentemann/Google Drive/public/2019_saildrone/saildrone_arctic_sd1037_2019.nc')\n",
    "#ds_tem2 = xr.open_dataset('f:/data/cruise_data/saildrone/2019_arctic/post_mission/saildrone-gen_5-arctic_misst_2019-sd1037-20190514T230000-20191011T183000-1_minutes-v1.1575487464625.nc')\n",
    "#ds_tem = ds_tem.isel(row=slice(60,-1))\n",
    "#ds_tem2 = ds_tem2.isel(obs=slice(60*24,-1))\n",
    "#print(ds_tem.time[0].data,ds_tem.time[-1].data)\n",
    "#print(ds_tem2.time[0,0].data,ds_tem2.time[0,-1].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv = data_dict['saildrone_west_coast_survey_2019_sd1047']\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds.cellon[jj,ii],ds.cellat[jj,ii,0],'b*')\n",
    "ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What lon range for satellite & insitu? are we going 0-360 or -180 to 180?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cellon.min().data,ds.cellon.max().data)\n",
    "print(ds_usv.lon.min().data,ds_usv.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's figure out what orbital files actually have data in our area of interest.  To do this, use the pyresample software\n",
    "\n",
    "- read in the in situ data\n",
    "- calculate the in situ min/max dates to know what files to check\n",
    "\n",
    "Now we have our time of interest\n",
    "\n",
    "- loop through the satellite data\n",
    "- calculate the in situ min/max lat/lon on the same day to define a small box of interest\n",
    "- use pyresample to map the data onto a predefined 0.1 deg resolution spatial grid\n",
    "- subset the gridded map to the area of interest\n",
    "- see if there is any valid data in that area\n",
    "- if there is any valid data, go to next step\n",
    "\n",
    "## Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def = load_area('areas.cfg', 'pc_world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effort to combine the finding & collocating code\n",
    "#intialize grid\n",
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "#for name in data_dict:\n",
    "for iname,name in enumerate(data_dict):\n",
    "#    if iname<23:\n",
    "#        continue\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,1):\n",
    "\n",
    "        ds_usv,name_usv=data_dict[name],name\n",
    "\n",
    "        if isat==0:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave3.nc'\n",
    "        if isat==1:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave3.nc'   \n",
    "\n",
    "#        if path.exists(fileout):\n",
    "#            continue\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(iname,name)\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            filelist = get_filelist_l2p(isat, usv_day)\n",
    "            x,y,z = [],[],[]\n",
    "            for ifile,file in enumerate(filelist):\n",
    "#                if ifile!=7:\n",
    "#                    continue\n",
    "#            for file in filelist:\n",
    "                ds = xr.open_dataset(file)\n",
    "                ds.close()  \n",
    "                if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.isel(look=0)\n",
    "                    ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "                    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "                if isat==1:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.rename({'row_time':'time'})\n",
    "\n",
    "#first do a quick check using resample to project the orbit onto a grid \n",
    "#and quickly see if there is any data in the cruise area on that day\n",
    "#if there is, then continue to collocation\n",
    "\n",
    "                x = ds['lon'].fillna(-89).data \n",
    "                y = ds['lat'].fillna(-89).data \n",
    "                z = ds['smap_sss'].data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                \n",
    "                # Resample swath to a fixed 0.01 x 0.01 grid, represented by the variable grid_def:\n",
    "                # https://stackoverflow.com/questions/58065055/floor-and-ceil-with-number-of-decimals\n",
    "                #changed to be just the region of the usv cruise to make grid even smaller (hopefully)\n",
    "                #when working with global orbital data, work with usv BUT\n",
    "                #when working with granules use ds instead of ds_usv so you just do granule region\n",
    "                grid_def_lon_min = np.round(ds_day.lon.min().data - 0.5 * 10**(-2), 2)\n",
    "                grid_def_lon_max = np.round(ds_day.lon.max().data + 0.5 * 10**(-2), 2)\n",
    "                grid_def_lat_min = np.round(ds_day.lat.min().data - 0.5 * 10**(-2), 2)\n",
    "                grid_def_lat_max = np.round(ds_day.lat.max().data + 0.5 * 10**(-2), 2)\n",
    "                grid_def_lons = np.arange(grid_def_lon_min,grid_def_lon_max+0.1,0.1)\n",
    "                grid_def_lats = np.arange(grid_def_lat_max,grid_def_lat_min-0.1,-0.1)\n",
    "                grid_mesh_lons,grid_mesh_lats = np.meshgrid(grid_def_lons,grid_def_lats)\n",
    "                # Since we have the lon and lat values for the area, we define a grid instead of an area:\n",
    "                # https://pyresample.readthedocs.io/en/latest/geo_def.html#griddefinition\n",
    "                grid_def = GridDefinition(lons=grid_mesh_lons,lats=grid_mesh_lats)\n",
    "\n",
    "                result1 = resample_nearest(swath_def, data, grid_def, radius_of_influence=20000, fill_value=None)\n",
    "#                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':grid_def_lats,'lon':grid_def_lons},dims=('lat','lon'))\n",
    "#                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "\n",
    "                numdata = np.isfinite(da).sum()\n",
    "                if numdata<1:\n",
    "                    continue\n",
    "\n",
    "                #chekc this!!!! for salinity data, da goes from 90 to -90, so slice max,min\n",
    "                #print('lat:',maxlat,minlat)\n",
    "                #print('lon:',maxlon,minlon)\n",
    "#                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "#                num_obs = np.isfinite(subset).sum()\n",
    "#                if num_obs<1:  #no collocations so go to next orbit\n",
    "#                    continue\n",
    "\n",
    "                #stack xarray dataset then drop lon == nan\n",
    "                ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "                #drop nan\n",
    "                ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "                lats = ds_drop.lat.data\n",
    "                lons = ds_drop.lon.data\n",
    "                inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "                tree = spatial.KDTree(inputdata)\n",
    "#                ilen = ds_usv.time.size\n",
    "                #find indices for ds_usv that are within 12 hours of orbit max/min time\n",
    "                orbit_time = np.datetime64(ds.attrs['time_coverage_start'])-np.timedelta64(12,'h')\n",
    "                orbit_time2 = np.datetime64(ds.attrs['time_coverage_end'])+np.timedelta64(12,'h')\n",
    "                #the RSS salinity files have 2000-01-01 where there is missing data in orbit, so use global attributes instead\n",
    "                #orbit_time = ds.time.min().data-np.timedelta64(12,'h')   #CHANGED TO +-12 HR\n",
    "                #orbit_time2 = ds.time.max().data+np.timedelta64(12,'h')    \n",
    "                cond = (ds_usv.time.data>orbit_time) & (ds_usv.time.data<orbit_time2)\n",
    "                item = np.argwhere(cond)\n",
    "                if item.sum()<1:  #no data within 12 hr of orbit\n",
    "                    continue\n",
    "                for iusv_index in range(int(item[0]),int(item[-1])):\n",
    "#                    if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "#                        continue\n",
    "                    pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "            #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "                    tree.query(pts,k=1)\n",
    "                    i = tree.query(pts)[1]\n",
    "                    rdist = tree.query(pts)[0]\n",
    "                    #don't use matchups more than 25 km away\n",
    "                    if rdist>.25:\n",
    "                        continue\n",
    "                    #use .where to find the original indices of the matched data point\n",
    "                    #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "                    result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "                    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "                    if len(listOfCoordinates)==0:\n",
    "                        continue\n",
    "                    ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "                    if isat==0:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "                    if isat==1:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "                    if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                        ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                        ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                        ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                        ds_usv.smap_name[iusv_index]=file\n",
    "                        ds_usv.smap_dist[iusv_index]=rdist\n",
    "                        ds_usv.smap_ydim[iusv_index]=ii\n",
    "                        ds_usv.smap_xdim[iusv_index]=jj\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        ds_usv.to_netcdf(fileout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
