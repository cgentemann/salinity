{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from scipy import spatial\n",
    "sys.path.append('./subroutines/')\n",
    "from read_routines import read_usv, get_filelist_l2p,get_orbital_data_l2p\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "list_var = ['time','lat','lon','SOG_MEAN','COG_MEAN','HDB_MEAN','ROLL_FILTERED_MEAN','PITCH_FILTERED_MEAN',\n",
    "            'UWND_MEAN','VWND_MEAN','WWND_MEAN','GUST_WND_MEAN','TEMP_AIR_MEAN','RH_MEAN','BARO_PRES_MEAN',\n",
    "            'PAR_AIR_MEAN','TEMP_CTD_MEAN','SAL_CTD_MEAN','TEMP_RBR_MEAN','SAL_RBR_MEAN',\n",
    "            'TEMP_O2_RBR_MEAN']\n",
    "\n",
    "swapvar = {'TEMP_SBE37_MEAN':'TEMP_CTD_MEAN','SAL_SBE37_MEAN':'SAL_CTD_MEAN','SAL_MEAN':'SAL_CTD_MEAN',\n",
    "           'TEMP_O2_RBR_MEAN':'TEMP_O2_MEAN','TEMP_CTD_RBR_MEAN':'TEMP_RBR_MEAN'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read in list of saildrone files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [x for x in glob(dir_data_pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ifile,file in enumerate(files):\n",
    "    ds = xr.open_dataset(file).rename({'latitude':'lat','longitude':'lon'})\n",
    "    if any(v=='trajectory' for v in ds.dims.keys()):\n",
    "        ds = ds.isel(trajectory=0)\n",
    "    ds.close()\n",
    "    for v in ds.dims.keys():\n",
    "        ds = ds.swap_dims({v:'time'})\n",
    "    if ds.trajectory.size==1:\n",
    "        iusv = float(ds.trajectory.data)\n",
    "    else:\n",
    "        iusv = float(ds.trajectory[0].data)\n",
    "\n",
    "    #remove any duplicates in time, keep only first value\n",
    "    _, index = np.unique(ds['time'], return_index=True)\n",
    "    ds=ds.isel(time=index)\n",
    "    #renames some common variables to uniform name, drop variables not on list above\n",
    "    dssv = ds\n",
    "    for var in ds:\n",
    "        var2 = var\n",
    "        if swapvar.get(var): \n",
    "            ds = ds.rename({var:swapvar.get(var)})\n",
    "            var2 = swapvar.get(var)\n",
    "        if any(vv==var2 for vv in list_var):\n",
    "            ds #just a place holder does nothing\n",
    "        else:\n",
    "            ds = ds.drop(var2)\n",
    "    #check that there is a TEMP_CTD_MEAN, if not & temp_rbr_mean there, change it to temp_ctd_mean\n",
    "    if any(var=='TEMP_CTD_MEAN' for var in ds):\n",
    "        ds #just a place holder does nothing\n",
    "    else:\n",
    "        if any(var=='TEMP_RBR_MEAN' for var in ds):\n",
    "            ds = ds.rename({'TEMP_RBR_MEAN':'TEMP_CTD_MEAN'})\n",
    "    if any(var=='SAL_CTD_MEAN' for var in ds):\n",
    "        ds #just a place holder does nothing\n",
    "    else:\n",
    "        if any(var=='SAL_RBR_MEAN' for var in ds):\n",
    "            ds = ds.rename({'SAL_RBR_MEAN':'SAL_CTD_MEAN'})\n",
    "    if ds.attrs.get('project'):\n",
    "        pname = ds.attrs['project']\n",
    "    else:\n",
    "        pname = ds.attrs['id']\n",
    "    name = str(ds.time[0].dt.year.data)+'_'+str(int(iusv))+pname\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = name.replace(\"/\", \"_\")\n",
    "    print(name)\n",
    "    if ifile==0:\n",
    "        data_dict = {name:ds}\n",
    "    else:\n",
    "        data_dict[name]=ds\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example showing how the using matplotlib maps orbital data quickly and easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv = data_dict['2019_1039west_coast_survey_2019']\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds.cellon[jj,ii],ds.cellat[jj,ii,0],'b*')\n",
    "ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What lon range for satellite & insitu? are we going 0-360 or -180 to 180?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cellon.min().data,ds.cellon.max().data)\n",
    "print(ds_usv.lon.min().data,ds_usv.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# okay, change all datasets in data_dict to 0 to 360 to match satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_360 = data_dict.copy()\n",
    "for name in data_dict:\n",
    "    ds = data_dict[name]\n",
    "    ds.coords['lon'] = np.mod(ds['lon'], 360)\n",
    "    ds = ds.sortby(ds.lon)\n",
    "    data_dict_360[name]=ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv = data_dict_360['2017_1001saildrone_arctic_data_2fb5_c534_0538']\n",
    "print(ds_usv.lon.min().data,ds_usv.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's figure out what orbital files actually have data in our area of interest.  To do this, use the pyresample software\n",
    "\n",
    "- read in the in situ data\n",
    "- calculate the in situ min/max dates to know what files to check\n",
    "\n",
    "Now we have our time of interest\n",
    "\n",
    "- loop through the satellite data\n",
    "- calculate the in situ min/max lat/lon on the same day to define a small box of interest\n",
    "- use pyresample to map the data onto a predefined 0.1 deg resolution spatial grid\n",
    "- subset the gridded map to the area of interest\n",
    "- see if there is any valid data in that area\n",
    "- if there is any valid data, go to next step\n",
    "\n",
    "## Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effort to combine the finding & collocating code\n",
    "#intialize grid\n",
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "#for name in data_dict:\n",
    "for iname,name in enumerate(data_dict):\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,1):\n",
    "\n",
    "        ds_usv,name_usv=data_dict[name],name\n",
    "\n",
    "        if isat==0:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave3.nc'\n",
    "        if isat==1:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave3.nc'   \n",
    "\n",
    "#        if path.exists(fileout):\n",
    "#            continue\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(iname,name)\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            filelist = get_filelist_l2p(isat, usv_day)\n",
    "            x,y,z = [],[],[]\n",
    "            for file in filelist:\n",
    "                ds = xr.open_dataset(file)\n",
    "                ds.close()  \n",
    "                if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.isel(look=0)\n",
    "                    ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "                    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "                if isat==1:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.rename({'row_time':'time'})\n",
    "\n",
    "#first do a quick check using resample to project the orbit onto a grid \n",
    "#and quickly see if there is any data in the cruise area on that day\n",
    "#if there is, then continue to collocation\n",
    "\n",
    "                x = ds['lon'].fillna(-89).data \n",
    "                y = ds['lat'].fillna(-89).data \n",
    "                z = ds['smap_sss'].data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "                num_obs = np.isfinite(subset).sum()\n",
    "                if num_obs<1:  #no collocations so go to next orbit\n",
    "                    continue\n",
    "\n",
    "                #stack xarray dataset then drop lon == nan\n",
    "                ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "                #drop nan\n",
    "                ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "                lats = ds_drop.lat.data\n",
    "                lons = ds_drop.lon.data\n",
    "                inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "                tree = spatial.KDTree(inputdata)\n",
    "                orbit_time = ds.time.max().data-np.timedelta64(1,'D')\n",
    "                orbit_time2 = ds.time.max().data+np.timedelta64(1,'D')    \n",
    "                ilen = ds_usv.time.size\n",
    "                for iusv_index in range(200): #range(ilen):\n",
    "                    if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "                        continue\n",
    "                    pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "            #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "                    tree.query(pts,k=1)\n",
    "                    i = tree.query(pts)[1]\n",
    "                    rdist = tree.query(pts)[0]\n",
    "                    #don't use matchups more than 25 km away\n",
    "                    if rdist>.25:\n",
    "                        continue\n",
    "                    #use .where to find the original indices of the matched data point\n",
    "                    #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "                    result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "                    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "                    if len(listOfCoordinates)==0:\n",
    "                        continue\n",
    "                    ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "                    if isat==0:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "                    if isat==1:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "                    if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                        ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                        ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                        ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                        ds_usv.smap_name[iusv_index]=file\n",
    "                        ds_usv.smap_dist[iusv_index]=rdist\n",
    "                        ds_usv.smap_ydim[iusv_index]=ii\n",
    "                        ds_usv.smap_xdim[iusv_index]=jj\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        ds_usv.to_netcdf(fileout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
