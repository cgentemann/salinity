{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample.geometry import GridDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from scipy import spatial\n",
    "sys.path.append('../saildrone/subroutines/')\n",
    "from read_routines import read_all_usv, read_one_usv,add_coll_vars,get_filelist_l2p,get_orbital_data_l2p\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_data = 'C:/Users/gentemann/Google Drive/public/ALL_Saildrone_Data/' #'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'C:/Users/gentemann/Google Drive/public/ALL_Saildrone_Data/*west*.nc' \n",
    "#dir_data 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "#dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "#get list of all filenames in directory\n",
    "files = glob(dir_data_pattern)\n",
    "print('number of file:',len(files))\n",
    "#for ifile,file in enumerate(files):\n",
    "#    print(ifile,file)\n",
    "ds_usv = xr.open_dataset(files[4]).rename({'latitude':'lat','longitude':'lon'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv,name_usv = read_one_usv(files[0])\n",
    "usv_day = ds_usv.time[0]\n",
    "filelist_jpl = get_filelist_l2p(0, usv_day)\n",
    "filelist_rss = get_filelist_l2p(1, usv_day)\n",
    "\n",
    "file=filelist_jpl[0]\n",
    "ii = file.find('_r')\n",
    "iorb = int(file[ii+2:ii+7])\n",
    "file_rss=[tem for tem in filelist_rss if str(iorb) in tem]\n",
    "ds = xr.open_dataset(filelist_jpl[0])\n",
    "ds['cellon'] = (ds['cellon'] + 180) % 360 - 180\n",
    "ds.close()\n",
    "ds2 = xr.open_dataset(file_rss[0])\n",
    "ds2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "#minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "#ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds_usv.lon,ds_usv.lat,'ro')\n",
    "ax.coastlines()\n",
    "#ax.set_xlim(-130,-100)\n",
    "#ax.set_ylim(20,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ds2.lon.data\n",
    "y = ds2.lat.data\n",
    "z = ds2.smap_sss.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "#minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "#ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds_usv.lon,ds_usv.lat,'ro')\n",
    "ax.coastlines()\n",
    "#ax.set_xlim(-130,-100)\n",
    "#ax.set_ylim(20,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = 'C:/Users/gentemann/Google Drive/public/ALL_Saildrone_Data/' #'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "dir_data_pattern = 'C:/Users/gentemann/Google Drive/public/ALL_Saildrone_Data/*west*.nc' \n",
    "#dir_data 'f:/data/cruise_data/saildrone/saildrone_data/'\n",
    "#dir_data_pattern = 'f:/data/cruise_data/saildrone/saildrone_data/*.nc'\n",
    "\n",
    "#get list of all filenames in directory\n",
    "files = [x for x in glob(dir_data_pattern)]\n",
    "print('number of file:',len(files))\n",
    "for ifile,file in enumerate(files):\n",
    "    print(ifile,file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example showing how the using matplotlib maps orbital data quickly and easily\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_tem = xr.open_dataset('C:/Users/gentemann/Google Drive/public/2019_saildrone/saildrone_arctic_sd1037_2019.nc')\n",
    "#ds_tem2 = xr.open_dataset('f:/data/cruise_data/saildrone/2019_arctic/post_mission/saildrone-gen_5-arctic_misst_2019-sd1037-20190514T230000-20191011T183000-1_minutes-v1.1575487464625.nc')\n",
    "#ds_tem = ds_tem.isel(row=slice(60,-1))\n",
    "#ds_tem2 = ds_tem2.isel(obs=slice(60*24,-1))\n",
    "#print(ds_tem.time[0].data,ds_tem.time[-1].data)\n",
    "#print(ds_tem2.time[0,0].data,ds_tem2.time[0,-1].data)\n",
    "#ds\n",
    "#ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv = xr.open_dataset(files[4]).rename({'latitude':'lat','longitude':'lon'})\n",
    "#file = 'F:/data/sat_data/smap/SSS/L2/JPL/V4.3/2016/002/SMAP_L2B_SSS_04909_20160102T044855_R16010_V4.3.h5'\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds.cellon[jj,ii],ds.cellat[jj,ii,0],'b*')\n",
    "#ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.plot(ds_usv.lon,ds_usv.lat,'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test read in a file to look at dimension names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'F:/data/sat_data/smap/SSS/L2/JPL/V4.3/2016/002/SMAP_L2B_SSS_04909_20160102T044855_R16010_V4.3.h5'\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V4/SCI/2018/115/RSS_SMAP_SSS_L2C_r17250_20180425T004136_2018115_FNL_V04.0.nc'\n",
    "ds = xr.open_dataset(file,decode_cf=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What lon range for satellite & insitu? are we going 0-360 or -180 to 180?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.cellon.min().data,ds.cellon.max().data)\n",
    "print(ds_usv.lon.min().data,ds_usv.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's figure out what orbital files actually have data in our area of interest.  To do this, use the pyresample software\n",
    "\n",
    "- read in the in situ data\n",
    "- calculate the in situ min/max dates to know what files to check\n",
    "\n",
    "Now we have our time of interest\n",
    "\n",
    "- loop through the satellite data\n",
    "- calculate the in situ min/max lat/lon on the same day to define a small box of interest\n",
    "- use pyresample to map the data onto a predefined 0.1 deg resolution spatial grid\n",
    "- subset the gridded map to the area of interest\n",
    "- see if there is any valid data in that area\n",
    "- if there is any valid data, go to next step\n",
    "\n",
    "## Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_start_end(isat,ds):\n",
    "    if isat==0:\n",
    "        orbit_time = np.datetime64(ds.attrs['time_coverage_start'])-np.timedelta64(24,'h') #changed to 24 hr for sss\n",
    "        orbit_time2 = np.datetime64(ds.attrs['time_coverage_end'])+np.timedelta64(24,'h')  \n",
    "    if isat==1:\n",
    "        orbit_time = ds.time[0].data-np.timedelta64(12,'h')\n",
    "        orbit_time2 = ds.time[-1].data+np.timedelta64(12,'h')        \n",
    "    return orbit_time,orbit_time2\n",
    "\n",
    "area_def = load_area('areas.cfg', 'pc_world')\n",
    "rlon=np.arange(-180,180,.1)\n",
    "rlat=np.arange(90,-90,-.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iname = 1  #set number of cruise to process\n",
    "#for isat in range(2):\n",
    "\n",
    "ds_usv,name_usv = read_one_usv(files[iname])\n",
    "\n",
    "ds_usv = add_coll_vars_ds_jplrss(ds_usv)\n",
    "fileout_rss = 'F:/data/cruise_data/saildrone/sss/2sat_sss_collocations_orbital/'+name_usv+'jplv04.3_rssv04.0_orbital.nc'\n",
    "\n",
    "#search usv data\n",
    "minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "usv_day = minday\n",
    "print(iname,name_usv)\n",
    "print(minday.data,maxday.data)\n",
    "while usv_day<=maxday:\n",
    "    print(usv_day.data,maxday.data)\n",
    "    ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "    ilen = ds_day.time.size\n",
    "    if ilen<1:   #don't run on days without any data\n",
    "        usv_day += np.timedelta64(1,'D')\n",
    "        continue\n",
    "    minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "    filelist_jpl = get_filelist_l2p(0, usv_day)\n",
    "    filelist_rss = get_filelist_l2p(1, usv_day)\n",
    "    x,y,z = [],[],[]\n",
    "    for ifile,file in enumerate(filelist_jpl):\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds.close()  \n",
    "        #print('****************')\n",
    "        #print(file)\n",
    "\n",
    "        #find corresponding RSS file\n",
    "        file=filelist_jpl[0]\n",
    "        ii = file.find('_r')\n",
    "        iorb = int(file[ii+2:ii+7])\n",
    "        file_rss=[tem for tem in filelist_rss if str(iorb) in tem]\n",
    "\n",
    "        #change JPL data\n",
    "        ds = ds.rename({'row_time':'time','ice_concentration':'fice'})         \n",
    "\n",
    "#first do a quick check using resample to project the orbit onto a grid \n",
    "#and quickly see if there is any data in the cruise area on that day\n",
    "#if there is, then continue to collocation\n",
    "\n",
    "        x = ds['lon'].fillna(-89).data \n",
    "        y = ds['lat'].fillna(-89).data \n",
    "        z = ds['smap_sss'].data \n",
    "        lons,lats,data = x,y,z \n",
    "        swath_def = SwathDefinition(lons, lats)\n",
    "\n",
    "        # Resample swath to a fixed 0.01 x 0.01 grid, represented by the variable grid_def:\n",
    "        # https://stackoverflow.com/questions/58065055/floor-and-ceil-with-number-of-decimals\n",
    "        #changed to be just the region of the usv cruise to make grid even smaller (hopefully)\n",
    "        #when working with global orbital data, work with usv BUT\n",
    "        #when working with granules use ds instead of ds_usv so you just do granule region\n",
    "        grid_def_lon_min, grid_def_lon_max = np.round(ds_day.lon.min().data - 0.5 * 10**(-2), 2), np.round(ds_day.lon.max().data + 0.5 * 10**(-2), 2)\n",
    "        grid_def_lat_min, grid_def_lat_max = np.round(ds_day.lat.min().data - 0.5 * 10**(-2), 2), np.round(ds_day.lat.max().data + 0.5 * 10**(-2), 2)\n",
    "        grid_def_lons, grid_def_lats = np.arange(grid_def_lon_min,grid_def_lon_max+0.1,0.1), np.arange(grid_def_lat_max,grid_def_lat_min-0.1,-0.1)\n",
    "        grid_mesh_lons,grid_mesh_lats = np.meshgrid(grid_def_lons,grid_def_lats)\n",
    "\n",
    "        # Since we have the lon and lat values for the area, we define a grid instead of an area:\n",
    "        # https://pyresample.readthedocs.io/en/latest/geo_def.html#griddefinition\n",
    "        grid_def = GridDefinition(lons=grid_mesh_lons,lats=grid_mesh_lats)\n",
    "\n",
    "        result1 = resample_nearest(swath_def, data, grid_def, radius_of_influence=20000, fill_value=None)\n",
    "        da = xr.DataArray(result1,name='sss',coords={'lat':grid_def_lats,'lon':grid_def_lons},dims=('lat','lon'))\n",
    "\n",
    "        numdata = np.isfinite(da).sum()\n",
    "        if numdata<1:\n",
    "            continue\n",
    "\n",
    "        #stack xarray dataset then drop lon == nan\n",
    "        ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "        #drop nan\n",
    "        ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "        lats = ds_drop.lat.data\n",
    "        lons = ds_drop.lon.data\n",
    "        inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "        tree = spatial.KDTree(inputdata)\n",
    "\n",
    "        orbit_time, orbit_time2 = get_time_start_end(isat,ds)\n",
    "\n",
    "        cond = (ds_usv.time.data>orbit_time) & (ds_usv.time.data<orbit_time2)\n",
    "        item = np.argwhere(cond)\n",
    "        if item.sum()<1:  #no data within 12 hr of orbit\n",
    "            continue\n",
    "        for iusv_index in range(int(item[0]),int(item[-1])):\n",
    "            pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]]) #pts = np.array([ds_usv.lon[iusv]+360\n",
    "            tree.query(pts,k=1)\n",
    "            i = tree.query(pts)[1]\n",
    "            rdist = tree.query(pts)[0]                   \n",
    "            if rdist>.25:    #don't use matchups more than 25 km away\n",
    "                continue\n",
    "            #use .where to find the original indices of the matched data point\n",
    "            #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "            result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "            listOfCoordinates = list(zip(result[0], result[1]))\n",
    "            if len(listOfCoordinates)==0:\n",
    "                continue\n",
    "            ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "#                if isat==0:\n",
    "            deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "#                if isat==1:\n",
    "#                    deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "            if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                ds_usv.smap_rev_number[iusv_index]=int(ds.attrs['REVNO'])\n",
    "                ds_usv.smap_iqc_flag_jpl[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                ds_usv.smap_name_jpl[iusv_index]=str(file)\n",
    "                ds_usv.smap_fice_jpl[iusv_index]=ds.fice[ii,jj]\n",
    "                ds_usv.smap_dist_jpl[iusv_index]=rdist\n",
    "                ds_usv.smap_ydim_jpl[iusv_index]=ii\n",
    "                ds_usv.smap_xdim_jpl[iusv_index]=jj\n",
    "\n",
    "                #match with JPL found, now get RSS data\n",
    "                mlat = ds.lat[ii,jj]\n",
    "                mlon = ds.lon[ii,jj]\n",
    "                ds2 = xr.open_dataset(file_rss[0])\n",
    "                ds2.close()\n",
    "                dist = ((ds2.cellat.isel(look=0)-mlat)**2+(ds2.cellon.isel(look=0)-mlon)**2)**.5\n",
    "                dd = ds2.isel(dist.argmin(dim=[\"xdim_grid\", \"ydim_grid\"]))\n",
    "\n",
    "                ds_usv.smap_SSS_rss_40km[iusv_index]=dd.smap_sss_40km.mean('look')                  \n",
    "                ds_usv.smap_SSS_rss[iusv_index]=dd.smap_sss.mean('look')\n",
    "                ds_usv.smap_iqc_flag_rss[iusv_index]=dd.quality_flag.mean('look')\n",
    "                ds_usv.smap_fice_rss[iusv_index]=dd.fice.mean('look')\n",
    "                ds_usv.smap_fland_rss[iusv_index]=dd.fland.mean('look')\n",
    "\n",
    "    usv_day += np.timedelta64(1,'D')\n",
    "ds_usv.to_netcdf(fileout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename files from filesave4 to orbital since later I did 8day collocations as well, \n",
    "import os\n",
    "dir_data_pattern = 'F:/data/cruise_data/saildrone/sss/2sat_sss_collocations_orbital_norepeat/*.nc' \n",
    "files = glob(dir_data_pattern)\n",
    "print('number of file:',len(files))\n",
    "for file in files:\n",
    "    file2 = file.replace('filesave4','orbital')\n",
    "    print(file,file2)\n",
    "    os.rename(file,file2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
