{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocate MUR SST with SMAP salinity & Saildrone\n",
    "-this reads in the MUR SST from AWS PODAAC collocates it with all Saildrone cruises that have already been collocated with SMAP salinity\n",
    "- The code stops running about every 12 files. It doesn't break but it starts returning nan for SSTs that is collocating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in All Saildrone cruises downloaded from https://data.saildrone.com/data/sets\n",
    "- 2017 onwards, note that earlier data is going to lack insruments and be poorer data quality in general\n",
    "- For this code I want to develop a routine that reads in all the different datasets and creates a standardized set\n",
    "- It may work best to first read each of the files individually into a dictionary \n",
    "- then go through each dataset finding all variable names\n",
    "- I decided to put all SST into TEMP_CTD_MEAN and same for Salinity so there is a single variable name\n",
    "- this still preserves all the dataset information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now gridded no repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "from scipy import spatial\n",
    "#sys.path.append('/home/jovyan/shared/users/cgentemann/notebooks/salinity/subroutines/')\n",
    "#from read_routines import read_all_usv, read_one_usv, add_coll_vars\n",
    "import warnings\n",
    "warnings.simplefilter('ignore') # filter some warning messages\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# these libraries help reading cloud data\n",
    "import fsspec \n",
    "import s3fs\n",
    "import requests\n",
    "import os\n",
    "\n",
    "warnings.simplefilter(\"ignore\")  # filter some warning messages\n",
    "xr.set_options(display_style=\"html\",keep_attrs=True)  # display dataset nicely\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data_pattern = '/home/jovyan/data/sss_collocations_8day_nearest_norepeat/'\n",
    "dir_out =         '/home/jovyan/data/sss_collocations_8day_nearest_norepeat_mur/'\n",
    "files = glob(dir_data_pattern+'*.nc')\n",
    "for ifile,file in enumerate(files):\n",
    "    ds = xr.open_dataset(file)\n",
    "    ds.close()\n",
    "    if any(v=='ob' for v in ds.dims.keys()):\n",
    "        ds = ds.swap_dims({'ob':'time'})\n",
    "    #remove any duplicates in time, keep only first value\n",
    "    _, index = np.unique(ds['time'], return_index=True)\n",
    "    ds=ds.isel(time=index)\n",
    "    name = file[57:-3]\n",
    "    name = name.replace(\" \", \"_\")\n",
    "    name = name.replace(\"/\", \"_\")\n",
    "    if ifile==0:\n",
    "        data_dict = {name:ds}\n",
    "    else:\n",
    "        data_dict[name]=ds\n",
    "    print(ifile,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iname,name in enumerate(data_dict):\n",
    "    fout = dir_out+name+'_20211116.nc'\n",
    "    ds_usv = xr.open_dataset(fout)\n",
    "    ds_usv.close()\n",
    "    print(iname,ds_usv.analysed_sst.mean().data)\n",
    "    #plt.show()\n",
    "    #plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.environ['HOME'],'shared','users','lib'))\n",
    "import ebdpy as ebd\n",
    "\n",
    "ebd.set_credentials(profile='esip-qhub')\n",
    "\n",
    "profile = 'esip-qhub'\n",
    "region = 'us-west-2'\n",
    "endpoint = f's3.{region}.amazonaws.com'\n",
    "ebd.set_credentials(profile=profile, region=region, endpoint=endpoint)\n",
    "worker_max = 30\n",
    "client,cluster = ebd.start_dask_cluster(profile=profile,worker_max=worker_max, \n",
    "                                      region=region, use_existing_cluster=True,\n",
    "                                      adaptive_scaling=False, wait_for_cluster=False, \n",
    "                                      environment='pangeo', worker_profile='Medium Worker', \n",
    "                                      propagate_env=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from earthdata import Auth \n",
    "auth = Auth().login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.podaac.earthdata.nasa.gov/s3credentials\"\n",
    "response = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# set up read\n",
    "json_consolidated = \"s3://esip-qhub-public/nasa/mur/murv41_consolidated_20211011.json\"\n",
    "s_opts = {\"requester_pays\": True, \"skip_instance_cache\": True}\n",
    "r_opts = {\"key\": response[\"accessKeyId\"],\"secret\": response[\"secretAccessKey\"],\"token\": response[\"sessionToken\"],\"client_kwargs\": {\"region_name\": \"us-west-2\"},}\n",
    "fs = fsspec.filesystem(\"reference\",fo=json_consolidated,\n",
    "                       ref_storage_args=s_opts,remote_protocol=\"s3\",\n",
    "                       remote_options=r_opts,simple_templates=True,)\n",
    "ds_sst = xr.open_dataset(fs.get_mapper(\"\"), decode_times=False, engine=\"zarr\", consolidated=False)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds_sst\n",
    "for iname,name in enumerate(data_dict):\n",
    "    if iname<87:\n",
    "        continue\n",
    "    print(iname,name)\n",
    "    ds_usv = data_dict[name]\n",
    "    #create space for new data\n",
    "    for var in ds_sst:  \n",
    "        ds_usv[var]=ds_usv.BARO_PRES_MEAN.copy(deep=True)*np.nan\n",
    "        ds_usv[var].attrs=ds_sst[var].attrs\n",
    "    ilen = len(ds_usv.time)\n",
    "    for inc in range(0,ilen,5):\n",
    "        #print(inc)\n",
    "        i1,i2 = inc,inc+5\n",
    "        if i2>ilen:\n",
    "            i2=ilen-1\n",
    "        if i1==i2:\n",
    "            continue\n",
    "        #print(inc,inc+101)\n",
    "        sub = ds_usv.isel(time=slice(i1,i2))   \n",
    "        t1,t2=sub.time.min().data-np.timedelta64(1,'D'),sub.time.max().data+np.timedelta64(1,'D')\n",
    "        x1,x2=sub.lon.min().data-.15,sub.lon.max().data+.15\n",
    "        y1,y2=sub.lat.min().data-.15,sub.lat.max().data+.15\n",
    "        #print(inc,t1,t2,x1,x2,y1,y2)\n",
    "        ds_sat = ds_sst.sel(time=slice(t1,t2),lat=slice(y1,y2),lon=slice(x1,x2))  \n",
    "        ds_sat['time']=np.asarray(ds_sat.time.data, \"datetime64[ns]\") \n",
    "        ds_interp = ds_sat.interp(time=sub.time,lat=sub.lat,lon=sub.lon,method='linear',assume_sorted=False) #add saildrone data to interpolated sat data\n",
    "        #add saildrone data to interpolated sat data\n",
    "        ds_interp = ds_interp.reset_coords(names={'lat','lon'})\n",
    "        for var in ds_interp:\n",
    "            ds_usv[var][i1:i2]=ds_interp[var]        \n",
    "    #output\n",
    "    fout = dir_out+name+'_20211116.nc'\n",
    "    ds_usv.to_netcdf(fout)\n",
    "    val = ds_usv.analysed_sst.mean().data\n",
    "    if np.isnan(val):\n",
    "        print(iname,val)\n",
    "        print('STOP')\n",
    "        break\n",
    "    print('output done, start new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv.to_netcdf(fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_interp = ds_sat.interp(time=sub.time).load()\n",
    "#ds_interp = ds_interp.reset_coords(names={'lat','lon'})\n",
    "#ds_interp.analysed_sst.plot()\n",
    "#ds_interp = ds_interp.drop('ob')\n",
    "ds_interp.analysed_sst[0,:,:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.analysed_sst[5000,0:1000,18000:19000].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sst.analysed_sst[5000,9000,18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tricky bit here, .interp wasn't working\n",
    "- ds_sat is being read somewhere as \"datetime64[us]\" rather than \"datetime64[ns]\"\n",
    "- this is breaking the interpolation routine which expects \"datetime64[ns]\"\n",
    "- solution is to set ds_sat time to \"datetime64[ns]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sat.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(ds_sat.time.data, \"datetime64[ns]\") \n",
    "ds_sat['time']=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem2 = ds_sat.interp(time=ds_usv.time,lat=ds_usv.lat,lon=ds_usv.lon,method='linear',assume_sorted=False)\n",
    "#tem2 = ds_sat.sel(time=ds_sat.time[1],method='nearest')#,lat=ds_usv.lat[0],lon=ds_usv.lon[0],method='linear',assume_sorted=False)\n",
    "#tem2 = ds_sat.sel(time=ds_usv.time[0],tem2 = ds_sat.sel(time=ds_sat.time[1],method='nearest')#,lat=ds_usv.lat[0],lon=ds_usv.lon[0],method='linear',assume_sorted=False)\n",
    "#tem2 = ds_sat.sel(time=data[0],method='nearest')#,lat=ds_usv.lat[0],lon=ds_usv.lon[0],method='linear',assume_sorted=False)\n",
    "#lat=ds_usv.lat[0],lon=ds_usv.lon[0],method='nearest')#,method='linear',assume_sorted=False)\n",
    "tem2.analysed_sst.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem2 = ds_sat.sel(time=sub.time,lat=sub.lat,lon=sub.lon,method='nearest') \n",
    "tem2.analysed_sst.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo]",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
