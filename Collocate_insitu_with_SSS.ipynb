{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the in situ and SSS collocation code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy\n",
    "from glob import glob\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from scipy import spatial\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "import sys\n",
    "sys.path.append('./subroutines/')\n",
    "from read_routines import read_usv, get_filelist_l2p,get_orbital_data_l2p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to read in insitu data\n",
    "- Read in the Saildrone USV file either from a local disc or using OpenDAP.\n",
    "- add room to write collocated data to in situ dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore the in situ data and quickly plot using cartopy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv,name=read_usv(adir,11)\n",
    "print(ds_usv.time.min().data,ds_usv.time.max().data)\n",
    "print(ds_usv.time.min().dt.dayofyear.data,ds_usv.time.max().dt.dayofyear.data)\n",
    "#ds_usv,name=read_usv(6)\n",
    "#print(ds_usv.time.min().dt.dayofyear.data,ds_usv.time.max().dt.dayofyear.data)\n",
    "#ds = xr.open_dataset('https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/insitu/L2/spurs2/saildrone/SPURS2_Saildrone1006.nc')\n",
    "ds_usv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_usv.time.plot()\n",
    "#subset = ds_usv.sel(time=slice('2018-01-01','2019-01-01'))\n",
    "#print(subset.time.dt.dayofyear)\n",
    "#print(ds_usv.time[0:100].dt.year,ds_usv.time[0:100].dt.dayofyear)\n",
    "#ds_usv.where(ds_usv['lon']>-180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "for iusv in range(8,12):\n",
    "    ds_usv,usvname=read_usv(adir,iusv)\n",
    "    #ds_usv,usvname = read_usv(iusv)\n",
    "    print(usvname)\n",
    "    #plot cruise SSS with coastlines\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "    #ds_usv = ds_usv.where(np.isfinite(ds_usv.lon))\n",
    "    cs1 = ax.scatter(ds_usv.lon, ds_usv.lat, s=3.0, c=ds_usv.SAL_MEAN, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "    ax.coastlines()\n",
    "    x1,x2,y1,y2 = ds_usv.lon.min().data-2,ds_usv.lon.max().data+2,ds_usv.lat.min().data-2,ds_usv.lat.max().data+2\n",
    "    ax.set_xlim(x1,x2)\n",
    "    ax.set_ylim(y1,y2)\n",
    "    ax.set_xticks(np.arange(x1,x2,4))\n",
    "    ax.set_yticks(np.arange(y1,y2,5))\n",
    "    cax = plt.colorbar(cs1)\n",
    "    cax.set_label('Salinity (psu)')\n",
    "    fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/'+usvname+'_location.png'\n",
    "    plt.savefig(fig_fname, transparent=False, format='png')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iusv)\n",
    "ds_usv.SAL_RBR_MEAN.plot()\n",
    "#ds_usv.SAL_SBE37_MEAN.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example showing how the using matplotlib maps orbital data quickly and easily\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "ds_usv,usvname=read_usv(adir,10)\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/2018/115/RSS_SMAP_SSS_L2C_40km_r17250_20180425T004136_2018115_FNL_V03.0.nc'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "x = ds.cellon.data\n",
    "y = ds.cellat.data\n",
    "z = ds.sss_smap.data\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(x, y, s=1.0, c=z, edgecolor='none', cmap='jet')\n",
    "minlon,maxlon,minlat,maxlat = ds_usv.lon.min().data,ds_usv.lon.max().data,ds_usv.lat.min().data,ds_usv.lat.max().data\n",
    "ax.plot([minlon,maxlon,maxlon,minlon,minlon],[minlat,minlat,maxlat,maxlat,minlat])\n",
    "#ax.plot(ds.cellon[jj,ii],ds.cellat[jj,ii,0],'b*')\n",
    "ax.plot(ds_usv.lon[1000],ds_usv.lat[1000],'ro')\n",
    "ax.coastlines()\n",
    "ax.set_xlim(-130,-110)\n",
    "ax.set_ylim(25,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But you can't search off of a scatter plot, so let's use pyresample to quickly project the orbit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_def = load_area('areas.cfg', 'pc_world')\n",
    "rlon=np.arange(-180,180,.1)\n",
    "rlat=np.arange(90,-90,-.1)\n",
    "file = 'F:/data/sat_data/smap/SSS/L2/RSS/V4/SCI/2018/115/RSS_SMAP_SSS_L2C_r17250_20180425T004136_2018115_FNL_V04.0.nc'\n",
    "#file = 'F:/data/sat_data/smap/SSS/L2/JPL/V4.2/2018/020/SMAP_L2B_SSS_15866_20180120T093138_R16010_V4.2.h5'\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "ds = ds.isel(look=0)\n",
    "x = (np.mod(ds.cellon+180,360)-180).data \n",
    "y = ds.cellat.data \n",
    "z = ds.sss_smap.data \n",
    "lons,lats,data = x,y,z \n",
    "swath_def = SwathDefinition(lons, lats)\n",
    "result = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "save_quicklook('C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/sss2.png', area_def, result, num_meridians=0, num_parallels=0, label='Salinity (psu)')\n",
    "#now plot on basemap\n",
    "bmap = area_def2basemap(area_def)\n",
    "bmng = bmap.bluemarble()\n",
    "col = bmap.imshow(result, origin='upper',vmin=32,vmax=35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's figure out what orbital files actually have data in our area of interest.  To do this, use the pyresample software\n",
    "\n",
    "- read in the in situ data\n",
    "- calculate the in situ min/max dates to know what files to check\n",
    "\n",
    "Now we have our time of interest\n",
    "\n",
    "- loop through the satellite data\n",
    "- calculate the in situ min/max lat/lon on the same day to define a small box of interest\n",
    "- use pyresample to map the data onto a predefined 0.1 deg resolution spatial grid\n",
    "- subset the gridded map to the area of interest\n",
    "- see if there is any valid data in that area\n",
    "- if there is any valid data, save the filename into a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isat=1\n",
    "flist = get_filelist_l2p(isat, ds_usv.time[0])\n",
    "flist\n",
    "print(flist[0])\n",
    "xlat,xlon,sat_time,var_data,sat_qc = get_orbital_data_l2p(isat,flist[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isat=0\n",
    "flist = get_filelist_l2p(isat, ds_usv.time[0])\n",
    "flist\n",
    "#xlat,xlon,sat_time,var_data,sat_qc = get_orbital_data_l2p(1,flist[0])\n",
    "#xlat\n",
    "file = flist[0]\n",
    "file.replace('\\\\', '/')\n",
    "ds = xr.open_dataset(file)\n",
    "ds.close()\n",
    "if isat==0:  #change RSS data to conform with JPL definitions\n",
    "    ds = ds.isel(look=0)\n",
    "    ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "if isat==1:  #change JPL data to conform with RSS definitions\n",
    "    ds = ds.rename({'row_time':'time'})\n",
    "xlat = ds['lat']\n",
    "xlon = ds['lon']\n",
    "var_data = ds['smap_sss']\n",
    "sat_time = ds['time']\n",
    "sat_qc = ds['quality_flag']\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.time[628:690,1000].plot()\n",
    "#ds.time.plot()\n",
    "ds.time[600,:].plot()\n",
    "ds.time[600,420:440]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#intialize grid\n",
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "for iusv in range(4,5):\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,1):\n",
    "\n",
    "        ds_usv,name_usv=read_usv(adir,iusv)\n",
    "\n",
    "        if isat==0:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave2.nc'\n",
    "        if isat==1:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave2.nc'   \n",
    "\n",
    "        if path.exists(fileout):\n",
    "            continue\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            filelist = get_filelist_l2p(isat, usv_day)\n",
    "            x,y,z = [],[],[]\n",
    "            for file in filelist:\n",
    "                xlat,xlon,sat_time,var_data,sat_qc = get_orbital_data_l2p(isat,file)\n",
    "                x = xlon.fillna(-89).data \n",
    "                y = xlat.fillna(-89).data \n",
    "                z = var_data.data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "                num_obs = np.isfinite(subset).sum()\n",
    "                if num_obs>0:\n",
    "                    file_save = np.append(file_save,file)\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        df = xr.DataArray(file_save,name='filenames')\n",
    "        df.to_netcdf(fileout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, loop through only the files that we know have some data in the region of interest.  Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_usv=0\n",
    "for isat in range(0,2):\n",
    "    for iusv in range(4,5):\n",
    "\n",
    "    ds_usv,name_usv=read_usv(adir,iusv)\n",
    "\n",
    "    if isat==0:\n",
    "        filelist = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave2.nc'\n",
    "        fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+usv_name+'rssv4_usv2.nc'\n",
    "    if isat==1:\n",
    "        filelist = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave2.nc'   \n",
    "        fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+usv_name+'jplv42_usv2.nc'   \n",
    "\n",
    "    df = xr.open_dataset(filelist)\n",
    "    df.close()\n",
    "    print(isat)\n",
    "    for file2 in df.filenames.data:\n",
    "        file = file2\n",
    "        file.replace('\\\\','/')\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds.close()  \n",
    "        if isat==0:  #change RSS data to conform with JPL definitions\n",
    "            ds = ds.isel(look=0)\n",
    "            ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "            ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "        if isat==1:  #change RSS data to conform with JPL definitions\n",
    "            ds = ds.rename({'row_time':'time'})\n",
    "        #stack xarray dataset then drop lon == nan\n",
    "        ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "        #drop nan\n",
    "        ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "        lats = ds_drop.lat.data\n",
    "        lons = ds_drop.lon.data\n",
    "        inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "        tree = spatial.KDTree(inputdata)\n",
    "        orbit_time = ds.time.max().data-np.timedelta64(1,'D')\n",
    "        orbit_time2 = ds.time.max().data+np.timedelta64(1,'D')    \n",
    " #       usv_subset = ds_usv.sel(time=slice(orbit_time,orbit_time2))\n",
    "        ilen = ds_usv.time.size\n",
    "        for iusv_index in range(ilen):\n",
    "            if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "                continue\n",
    "            pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "    #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "            tree.query(pts,k=1)\n",
    "            i = tree.query(pts)[1]\n",
    "            rdist = tree.query(pts)[0]\n",
    "            #don't use matchups more than 25 km away\n",
    "            if rdist>.25:\n",
    "                continue\n",
    "            #use .where to find the original indices of the matched data point\n",
    "            #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "            result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "            listOfCoordinates = list(zip(result[0], result[1]))\n",
    "            if len(listOfCoordinates)==0:\n",
    "                continue\n",
    "            ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "            if isat==0:\n",
    "                deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "            if isat==1:\n",
    "                deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "            if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                ds_usv.smap_name[iusv_index]=file2\n",
    "                ds_usv.smap_dist[iusv_index]=rdist\n",
    "                ds_usv.smap_ydim[iusv_index]=ii\n",
    "                ds_usv.smap_xdim[iusv_index]=jj\n",
    "    ds_usv.to_netcdf(fileout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine the previous two codes that were separate into one code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#effort to combine the finding & collocating code\n",
    "#intialize grid\n",
    "adir = 'C:/Users/gentemann/Google Drive/public/2019_saildrone/'\n",
    "for iusv in range(4,5):\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,1):\n",
    "\n",
    "        ds_usv,name_usv=read_usv(adir,iusv)\n",
    "\n",
    "        if isat==0:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'rssv4_filesave2.nc'\n",
    "        if isat==1:\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sss_collocations/'+name_usv+'jplv4.2_filesave2.nc'   \n",
    "\n",
    "        if path.exists(fileout):\n",
    "            continue\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            filelist = get_filelist_l2p(isat, usv_day)\n",
    "            x,y,z = [],[],[]\n",
    "            for file in filelist:\n",
    "                ds = xr.open_dataset(file)\n",
    "                ds.close()  \n",
    "                if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.isel(look=0)\n",
    "                    ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "                    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "                if isat==1:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.rename({'row_time':'time'})\n",
    "\n",
    "#first do a quick check using resample to project the orbit onto a grid \n",
    "#and quickly see if there is any data in the cruise area on that day\n",
    "#if there is, then continue to collocation\n",
    "                x = ds['lon'].fillna(-89).data \n",
    "                y = ds['lat'].fillna(-89).data \n",
    "                z = ds['smap_sss'].data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "                num_obs = np.isfinite(subset).sum()\n",
    "                if num_obs<1:  #no collocations so go to next orbit\n",
    "                    continue\n",
    "\n",
    "                #stack xarray dataset then drop lon == nan\n",
    "                ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "                #drop nan\n",
    "                ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "                lats = ds_drop.lat.data\n",
    "                lons = ds_drop.lon.data\n",
    "                inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "                tree = spatial.KDTree(inputdata)\n",
    "                orbit_time = ds.time.max().data-np.timedelta64(1,'D')\n",
    "                orbit_time2 = ds.time.max().data+np.timedelta64(1,'D')    \n",
    "         #       usv_subset = ds_usv.sel(time=slice(orbit_time,orbit_time2))\n",
    "                ilen = ds_usv.time.size\n",
    "                for iusv_index in range(ilen):\n",
    "                    if (ds_usv.time[iusv_index]<orbit_time) or (ds_usv.time[iusv_index]>orbit_time2):\n",
    "                        continue\n",
    "                    pts = np.array([ds_usv.lon[iusv_index], ds_usv.lat[iusv_index]])\n",
    "            #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "                    tree.query(pts,k=1)\n",
    "                    i = tree.query(pts)[1]\n",
    "                    rdist = tree.query(pts)[0]\n",
    "                    #don't use matchups more than 25 km away\n",
    "                    if rdist>.25:\n",
    "                        continue\n",
    "                    #use .where to find the original indices of the matched data point\n",
    "                    #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "                    result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "                    listOfCoordinates = list(zip(result[0], result[1]))\n",
    "                    if len(listOfCoordinates)==0:\n",
    "                        continue\n",
    "                    ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "                    if isat==0:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "                    if isat==1:\n",
    "                        deltaTa = ((ds_usv.time[iusv_index]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "                    if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv_index].data):\n",
    "                        ds_usv.deltaT[iusv_index]=deltaTa\n",
    "                        ds_usv.smap_SSS[iusv_index]=ds.smap_sss[ii,jj]\n",
    "                        ds_usv.smap_iqc_flag[iusv_index]=ds.quality_flag[ii,jj]\n",
    "                        ds_usv.smap_name[iusv_index]=file\n",
    "                        ds_usv.smap_dist[iusv_index]=rdist\n",
    "                        ds_usv.smap_ydim[iusv_index]=ii\n",
    "                        ds_usv.smap_xdim[iusv_index]=jj\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        ds_usv.to_netcdf(fileout)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A larger STD that isn't reflective of uncertainty in the observation\n",
    "The collocation above will result in multiple USV data points matched with a single satellite\n",
    "observation.    The USV is sampling every 1 min and approximately few meters, while the satellite\n",
    "is an average over a footprint that is interpolated onto a daily mean map.  While calculating the mean would results in a valid mean, the STD would be higher and consist of a component that reflects the uncertainty of the USV and the satellite and a component that reflects the natural variability in the region that is sampled by the USV\n",
    "\n",
    "Below we use the 'nearest' collocation results to identify when multiple USV data are collcated to\n",
    "a single satellite observation.\n",
    "This code goes through the data and creates averages of the USV data that match the single CCMP collocated value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for isat in range(2):\n",
    "    if isat==0:\n",
    "        file = 'F:/data/cruise_data/saildrone/sat_collocations/rss40km_usv2.nc'\n",
    "        fileout = 'F:/data/cruise_data/saildrone/sat_collocations/rss40km_usv2_norepeats.nc'\n",
    "    if isat==1:\n",
    "        file = 'F:/data/cruise_data/saildrone/sat_collocations/jplv42_usv2.nc'   \n",
    "        fileout = 'F:/data/cruise_data/saildrone/sat_collocations/jplv42_usv2_norepeats.nc'   \n",
    "    ds_usv=xr.open_dataset(file)\n",
    "    ds_usv.close()\n",
    "    ds_usv = ds_usv.where(ds_usv.smap_SSS<10000,np.nan)\n",
    "    ilen,index = ds_usv.dims['time'],0\n",
    "    ds_tem = ds_usv.copy(deep=True)\n",
    "    duu, duu2, duv1, duv2, dlat, dlon, dut = [],[],[],[],[],[],np.empty((),dtype='datetime64')\n",
    "    index=0\n",
    "    while index <= ilen-2:\n",
    "        index += 1\n",
    "        if np.isnan(ds_usv.smap_SSS[index]):\n",
    "            continue\n",
    "        if np.isnan(ds_usv.smap_xdim[index]):\n",
    "            continue\n",
    "        result = np.where((ds_usv.smap_xdim == ds_tem.smap_xdim[index].data) & (ds_usv.smap_ydim == ds_tem.smap_ydim[index].data))       \n",
    "        duu=np.append(duu,ds_usv.smap_SSS[result[0][0]].data)\n",
    "        duu2=np.append(duu2,ds_usv.smap_iqc_flag[result[0][0]].data)\n",
    "        duv1=np.append(duv1,ds_usv.SAL_MEAN[result].mean().data)\n",
    "        dlat=np.append(dlat,ds_usv.lat[result].mean().data)\n",
    "        dlon=np.append(dlon,ds_usv.lon[result].mean().data)\n",
    "        dut=np.append(dut,ds_usv.time[result].mean().data)\n",
    "        ds_usv.smap_SSS[result]=np.nan\n",
    "    dut2 = dut[1:]  #remove first data point which is a repeat from what array defined       \n",
    "    ds_new=xr.Dataset(data_vars={'smap_SSS': ('time',duu),'smap_iqc_flag': ('time',duu2),\n",
    "                                 'SAL_MEAN':('time',duv1),\n",
    "                                 'lon': ('time',dlon),\n",
    "                                 'lat': ('time',dlat)},\n",
    "                      coords={'time':dut2})\n",
    "    ds_new.to_netcdf(fileout)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now look at bias/std & plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files =['F:/data/cruise_data/saildrone/sat_collocations/rss40km_usv2_norepeats.nc',\n",
    "        'F:/data/cruise_data/saildrone/sat_collocations/jplv42_usv2_norepeats.nc',\n",
    "        'F:/data/cruise_data/saildrone/sat_collocations/rss40km_usv2.nc',\n",
    "        'F:/data/cruise_data/saildrone/sat_collocations/jplv42_usv2.nc']\n",
    "flabel=['RSS','JPL','RSS','JPL']\n",
    "for isat in range(4):\n",
    "    file = files[isat]\n",
    "    ds_usv=xr.open_dataset(file)\n",
    "    ds_usv = ds_usv.where(ds_usv.smap_SSS<10000,np.nan)\n",
    "    ds_usv = ds_usv.where(ds_usv.smap_iqc_flag<9400,np.nan)\n",
    "    if isat==1 or isat==3:\n",
    "        ds_usv = ds_usv.where(ds_usv.smap_iqc_flag<1,np.nan)\n",
    "    tem=ds_usv.smap_SSS-ds_usv.SAL_MEAN\n",
    "    tem = tem.where(np.isfinite(tem),drop=True)\n",
    "    print(flabel[isat],tem.mean().data,tem.std().data,tem.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just playing around & plotting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tem[20000].data,ds_usv.smap_iqc_flag[20000].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_usv.smap_SSS,'.')\n",
    "plt.plot(ds_usv.smap_iqc_flag,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_usv.smap_iqc_flag,ds_usv.smap_SSS-ds_usv.SAL_MEAN,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem=ds_usv.smap_SSS-ds_usv.SAL_MEAN\n",
    "plt.scatter(ds_usv.lon,ds_usv.lat,c=tem,vmin=-1,vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot cruise SSS with coastlines\n",
    "ax = plt.subplot(221,projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(ds_usv.lon, ds_usv.lat, s=3.0, c=ds_usv.SAL_MEAN, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,25,40\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "\n",
    "ax = plt.subplot(222,projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(ds_usv.lon, ds_usv.lat, s=3.0, c=ds_usv.smap_SSS, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,25,40\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/baja_location2.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileout = 'F:/data/cruise_data/saildrone/sat_collocations/jplv42_usv.nc'\n",
    "ds_usv = xr.open_dataset(fileout)\n",
    "tem = ds_usv.smap_SSS.where(ds_usv.smap_SSS<45,np.NaN)\n",
    "tem = tem.where(ds_usv.smap_SSS>28,np.NaN)\n",
    "print((tem-ds_usv.SAL_MEAN).mean().data,(tem-ds_usv.SAL_MEAN).std().data,(tem-ds_usv.SAL_MEAN).size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileout = 'F:/data/cruise_data/saildrone/sat_collocations/rss40km1_usv.nc'\n",
    "ds_usv = xr.open_dataset(fileout)\n",
    "tem = ds_usv.smap_SSS.where(ds_usv.smap_SSS<45,np.NaN)\n",
    "tem = tem.where(ds_usv.smap_SSS>28,np.NaN)\n",
    "print((tem-ds_usv.SAL_MEAN).mean().data,(tem-ds_usv.SAL_MEAN).std().data,(tem-ds_usv.SAL_MEAN).size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tem.time,tem)\n",
    "plt.plot(ds_usv.time,ds_usv.SAL_MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in ACCESS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'F:/data/cruise_data/access/rockfish_casts_2011.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds = ds.swap_dims({'row':'time'})\n",
    "ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds_access1 = ds.copy(deep=True)\n",
    "filename = 'F:/data/cruise_data/access/rockfish_casts_2015.nc'\n",
    "ds = xr.open_dataset(filename)\n",
    "ds = ds.swap_dims({'row':'time'})\n",
    "ds = ds.rename({'latitude':'lat','longitude':'lon'})\n",
    "ds_access2 = ds.copy(deep=True)\n",
    "ds_access = xr.concat((ds_access1,ds_access2),dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_access.time.dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ds_access.time.dt.year,ds_access.time.dt.dayofyear,'.')\n",
    "plt.xlabel('Day of Year')\n",
    "plt.ylabel('Year')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/access_date.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(ds_access.lon, ds_access.lat, s=3.0, c=ds_access.salinity, edgecolor='none', cmap='jet',vmin=33,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,30,50\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/access_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in 2018 Saildrone West Coast Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://coastwatch.pfeg.noaa.gov/erddap/\n",
    "#url = 'https://coastwatch.pfeg.noaa.gov/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018'\n",
    "#url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv['lat']=ds_usv['s.latitude']\n",
    "ds_usv['lon']=ds_usv['s.longitude']\n",
    "ds_usv['time']=ds_usv['s.time']\n",
    "ds_usv['trajectory']=ds_usv['s.trajectory']\n",
    "ds_usv['SAL_MEAN']=ds_usv['s.SAL_MEAN']\n",
    "ds_usv['TEMP_CTD_MEAN']=ds_usv['s.TEMP_CTD_MEAN']\n",
    "ds_usv = ds_usv.swap_dims({'s':'time'})\n",
    "#print(ds_usv.time.min().data,ds_usv.time.max().data)\n",
    "#ds_usv.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used this code to figure out where the data goes bad\n",
    "ilen = ds_usv.SAL_MEAN.size\n",
    "print(ilen)\n",
    "tem = np.nan(ilen)\n",
    "for i in range(787066):\n",
    "    if ds_usv.SAL_MEAN[i]>20:\n",
    "        tem[i]=ds_usv.SAL_MEAN[i]\n",
    "tem[i:]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there is something bad in the data file above 776100 so subset the data to just the good part\n",
    "ds_usv2 = ds_usv.isel(time=slice(None,776100))\n",
    "xlon =  ds_usv2.lon.copy(deep=True)\n",
    "xlat =  ds_usv2.lat.copy(deep=True)\n",
    "salinity = ds_usv2.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -130,-114,30,52\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/wcoast_location2.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv2.trajectory[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018'\n",
    "#url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "#url = 'https://ferret.pmel.noaa.gov/pmel/erddap/tabledap/saildrone_west_coast_survey_2018.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://ferret.pmel.noaa.gov/generic/erddap/tabledap/saildrone_arctic_gts'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "ds_usv['lat']=ds_usv['s.latitude']\n",
    "ds_usv['lon']=ds_usv['s.longitude']\n",
    "ds_usv['time']=ds_usv['s.time']\n",
    "ds_usv['trajectory']=ds_usv['s.trajectory']\n",
    "ds_usv['SAL_MEAN']=ds_usv['s.SAL_MEAN']\n",
    "ds_usv['TEMP_CTD_MEAN']=ds_usv['s.TEMP_CTD_MEAN']\n",
    "#ds_usv = ds_usv.swap_dims({'s':'time'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_usv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlon =  ds_usv.lon.copy(deep=True)\n",
    "xlat =  ds_usv.lat.copy(deep=True)\n",
    "salinity = ds_usv.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -130,-114,30,52\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/arctic_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'F:/data/cruise_data/saildrone/2018_wcoast/saildrone_west_coast_survey_2018_f374_2e74_3de8.nc'\n",
    "ds_usv = xr.open_dataset(url)\n",
    "ds_usv.close()\n",
    "xlon =  ds_usv.longitude.copy(deep=True)\n",
    "xlat =  ds_usv.latitude.copy(deep=True)\n",
    "salinity =  ds_usv.SAL_MEAN.copy(deep=True)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "cs1 = ax.scatter(xlon, xlat, s=3.0, c=salinity, edgecolor='none', cmap='jet',vmin=32,vmax=34.35)\n",
    "ax.coastlines()\n",
    "x1,x2,y1,y2 = -128,-112,25,40\n",
    "ax.set_xlim(x1,x2)\n",
    "ax.set_ylim(y1,y2)\n",
    "ax.set_xticks(np.arange(x1,x2,4))\n",
    "ax.set_yticks(np.arange(y1,y2,5))\n",
    "cax = plt.colorbar(cs1)\n",
    "cax.set_label('Salinity (psu)')\n",
    "fig_fname = 'C:/Users/gentemann/Google Drive/f_drive/docs/projects/SSS/figures/wcoast_location.png'\n",
    "plt.savefig(fig_fname, transparent=False, format='png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
