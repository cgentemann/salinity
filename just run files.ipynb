{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the in situ and SSS collocation code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import scipy\n",
    "from glob import glob\n",
    "import cartopy.crs as ccrs\n",
    "from pyresample.geometry import AreaDefinition\n",
    "from pyresample import image, geometry, load_area, save_quicklook, SwathDefinition, area_def2basemap\n",
    "from pyresample.kd_tree import resample_nearest\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from scipy import spatial\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a function to read in insitu data\n",
    "- Read in the Saildrone USV file either from a local disc or using OpenDAP.\n",
    "- add room to write collocated data to in situ dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_usv(iusv):\n",
    "\n",
    "    filename_usv_list = ['https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/insitu/L2/spurs2/saildrone/SPURS2_Saildrone1005.nc',\n",
    "                         'https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/insitu/L2/spurs2/saildrone/SPURS2_Saildrone1006.nc',\n",
    "                         'https://podaac-opendap.jpl.nasa.gov/opendap/allData/insitu/L2/saildrone/Baja/saildrone-gen_4-baja_2018-sd1002-20180411T180000-20180611T055959-1_minutes-v1.nc',\n",
    "                        'F:/data/cruise_data/access/CTD_casts_ALL_NASA_update_010819.xlsx',\n",
    "                        'F:/data/cruise_data/saildrone/noaa_arctic/saildrone_PMEL_Arctic_2015_126.nc',\n",
    "                        'F:/data/cruise_data/saildrone/noaa_arctic/saildrone_PMEL_Arctic_2016_126.nc',\n",
    "                        'F:/data/cruise_data/saildrone/noaa_arctic/saildrone_PMEL_Arctic_2016_128.nc',\n",
    "                        'F:/data/cruise_data/saildrone/noaa_arctic/saildrone_PMEL_Arctic_2015_128.nc']\n",
    "    name_usv_list = ['SPURS2_1005','SPURS2_1006','baja','access',\n",
    "                     'arctic2015_126',\n",
    "                     'arctic2016_126',\n",
    "                     'arctic2016_128',\n",
    "                     'arctic2015_128']   \n",
    "\n",
    "    filename_usv = filename_usv_list[iusv]\n",
    "    if iusv==3:\n",
    "        df = pd.read_excel(filename_usv, sheet_name='data')\n",
    "        ds_usv = df.to_xarray()\n",
    "        ds_usv = ds_usv.where(ds_usv.Depth==-2,drop=True)\n",
    "        ds_usv = ds_usv.swap_dims({'index':'Date'}).rename({'Date':'time','Longitude':'lon','Latitude':'lat','Salinity':'salinity'}).sortby('time')\n",
    "    elif iusv<3:\n",
    "        ds_usv = xr.open_dataset(filename_usv)\n",
    "        ds_usv.close()\n",
    "        if iusv==2:\n",
    "            ds_usv = ds_usv.isel(trajectory=0).swap_dims({'obs':'time'}).rename({'longitude':'lon','latitude':'lat','SAL_MEAN':'salinity'})\n",
    "            ds_usv = ds_usv.sel(time=slice('2018-04-12T02','2018-06-10T18')) #get rid of last part and first part where USV being towed\n",
    "        else:\n",
    "            ds_usv = ds_usv.rename({'longitude':'lon','latitude':'lat','sss':'salinity'})\n",
    "    elif iusv>3:\n",
    "        ds_usv = xr.open_dataset(filename_usv)\n",
    "        ds_usv.close()\n",
    "        ds_usv = ds_usv.isel(trajectory=0).swap_dims({'obs':'time'}).rename({'longitude':'lon','latitude':'lat','sal_mean':'salinity'})\n",
    "\n",
    "            #    ds_usv['lon'] = ds_usv.lon.interpolate_na(dim='time',method='linear') #there are 6 nan values\n",
    "#    ds_usv['lat'] = ds_usv.lat.interpolate_na(dim='time',method='linear')\n",
    "\n",
    "    #add room to write collocated data information\n",
    "    ilen = ds_usv.time.shape[0]\n",
    "    ds_usv['deltaT']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_SSS']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_name']=xr.DataArray(np.empty(ilen,dtype=str),coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_dist']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_ydim']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_xdim']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "    ds_usv['smap_iqc_flag']=xr.DataArray(np.ones(ilen)*999999,coords={'time':ds_usv.time},dims=('time'))\n",
    "\n",
    "    #subset data to SMAP observational period\n",
    "    ds_usv = ds_usv.sel(time=slice('2015-05-10','2018-12-31'))\n",
    "\n",
    "    return ds_usv,name_usv_list[iusv]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explore the in situ data and quickly plot using cartopy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename='F:/data/cruise_data/saildrone/west_coast/saildrone_west_coast_survey_2018_2506_7567_f05c.nc'\n",
    "#3filename='F:/data/cruise_data/saildrone/noaa_arctic/saildrone_PMEL_Arctic_2015_128.nc'\n",
    "#filename='https://podaac-opendap.jpl.nasa.gov:443/opendap/allData/insitu/L2/spurs2/saildrone/SPURS2_Saildrone1006.nc'\n",
    "#ds=xr.open_dataset(filename)\n",
    "#print(ds)\n",
    "#ds\n",
    "#plt.plot(ds.longitude,ds.latitude,'.')\n",
    "#print(ds)\n",
    "#print(ds.obs.min().data,ds.obs.max().data)\n",
    "#ds_usv = ds.swap_dims({'obs':'time'})\n",
    "#ds_usv = ds_usv.sel(time=slice('2015-05-10','2018-12-31'))\n",
    "#plt.plot(ds.time[9009:9385])\n",
    "#plt.plot(ds.obs[9000:9400])\n",
    "#print(ds.time[9008:9011])\n",
    "#print(ds.time[-10:].data)\n",
    "#import collections\n",
    "#print([item for item, count in collections.Counter(ds_usv.time.data).items() if count > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 SPURS2_1005\n",
      "2017-10-16T00:00:00.000000000 2017-11-17T00:00:00.000000000\n",
      "1 SPURS2_1006\n",
      "2017-10-16T00:00:00.000000000 2017-11-17T00:00:00.000000000\n",
      "2 baja\n",
      "2018-04-12T02:00:00.000000000 2018-06-10T18:59:00.000000000\n",
      "3 access\n",
      "2015-06-21T00:00:00.000000000 2018-09-28T00:00:00.000000000\n",
      "4 arctic2015_126\n",
      "2015-05-12T20:00:16.000000000 2015-07-28T19:58:16.000000000\n",
      "5 arctic2016_126\n",
      "2016-05-23T00:01:15.000000000 2016-09-03T18:02:15.000000000\n",
      "6 arctic2016_128\n",
      "2016-05-23T00:01:15.000000000 2016-09-03T18:02:15.000000000\n"
     ]
    }
   ],
   "source": [
    "for iusv in range(7):\n",
    "    ds_usv,name_usv = read_usv(iusv)\n",
    "    print(iusv,name_usv)\n",
    "#    print(ds_usv.time.min().data,ds_usv.time.max().data)\n",
    "    ds_usv = ds_usv.sel(time=slice('2015-05-10','2018-12-31'))\n",
    "    print(ds_usv.time.min().data,ds_usv.time.max().data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004-05-21T00:00:00.000000000 2018-09-28T00:00:00.000000000\n"
     ]
    }
   ],
   "source": [
    "#intialize grid\n",
    "for iusv in range(7):\n",
    "    area_def = load_area('areas.cfg', 'pc_world')\n",
    "    rlon=np.arange(-180,180,.1)\n",
    "    rlat=np.arange(90,-90,-.1)\n",
    "\n",
    "    for isat in range(0,2):\n",
    "\n",
    "        ds_usv,name_usv = read_usv(iusv)\n",
    "\n",
    "        if isat==0:\n",
    "            sat_directory = 'F:/data/sat_data/smap/SSS/L2/RSS/V3/40km/'\n",
    "    #        sat_directory = 'Z:/SalinityDensity/smap/L2/RSS/V3/SCI/40KM/'\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+name_usv+'_rss40km_filesave2.nc'\n",
    "            file_end = '/*.nc'\n",
    "        if isat==1:\n",
    "            sat_directory = 'F:/data/sat_data/smap/SSS/L2/JPL/V4.2/'\n",
    "    #        sat_directory = 'Z:/SalinityDensity/smap/L2/JPL/V4.2/'\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+name_usv+'_jplv4.2_filesave2.nc'   \n",
    "            file_end = '/*.h5'\n",
    "\n",
    "        if path.exists(fileout):\n",
    "            continue\n",
    "        #init filelist\n",
    "        file_save=[]\n",
    "\n",
    "        #search usv data\n",
    "        minday,maxday = ds_usv.time[0],ds_usv.time[-1]\n",
    "        usv_day = minday\n",
    "        print(minday.data,maxday.data)\n",
    "        while usv_day<=maxday:\n",
    "    #        check_day = np.datetime64(str(usv_day.dt.year.data)+'-'+str(usv_day.dt.month.data).zfill(2)+'-'+str(usv_day.dt.day.data).zfill(2))\n",
    "    #        usv_day1 = usv_day + np.timedelta64(1,'D')\n",
    "    #        check_day1 = np.datetime64(str(usv_day1.dt.year.data)+'-'+str(usv_day1.dt.month.data).zfill(2)+'-'+str(usv_day1.dt.day.data).zfill(2))\n",
    "    #        ds_day = ds_usv.sel(time=slice(check_day,check_day1))\n",
    "            ds_day = ds_usv.sel(time=slice(usv_day-np.timedelta64(1,'D'),usv_day+np.timedelta64(1,'D')))\n",
    "            ilen = ds_day.time.size\n",
    "            if ilen<1:   #don't run on days without any data\n",
    "                continue\n",
    "            minlon,maxlon,minlat,maxlat = ds_day.lon.min().data,ds_day.lon.max().data,ds_day.lat.min().data,ds_day.lat.max().data\n",
    "            #caluclate filelist\n",
    "            filelist = glob(sat_directory+str(usv_day.dt.year.data)+'/'+str(usv_day.dt.dayofyear.data)+file_end)   \n",
    "            x,y,z = [],[],[]\n",
    "            for file in filelist:\n",
    "                file.replace('\\\\','/')\n",
    "                ds = xr.open_dataset(file)\n",
    "                ds.close()\n",
    "                if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                    ds = ds.isel(look=0)\n",
    "                    ds = ds.rename({'cellon':'lon','cellat':'lat','sss_smap':'smap_sss'})\n",
    "                    ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "                x = ds.lon.fillna(-89).data \n",
    "                y = ds.lat.fillna(-89).data \n",
    "                z = ds.smap_sss.data \n",
    "                lons,lats,data = x,y,z \n",
    "                swath_def = SwathDefinition(lons, lats)\n",
    "                result1 = resample_nearest(swath_def, data, area_def, radius_of_influence=20000, fill_value=None)\n",
    "                da = xr.DataArray(result1,name='sss',coords={'lat':rlat,'lon':rlon},dims=('lat','lon'))\n",
    "                subset = da.sel(lat = slice(maxlat,minlat),lon=slice(minlon,maxlon))\n",
    "                num_obs = np.isfinite(subset).sum()\n",
    "                if num_obs>0:\n",
    "                    file_save = np.append(file_save,file)\n",
    "            usv_day += np.timedelta64(1,'D')\n",
    "        df = xr.DataArray(file_save,name='filenames')\n",
    "        df.to_netcdf(fileout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, loop through only the files that we know have some data in the region of interest.  Use the fast search kdtree which is part of pyresample software, but I think maybe comes originally from sci-kit-learn.\n",
    "\n",
    "- read in the in situ data\n",
    "- read in a single orbit of satellite data\n",
    "- kdtree can't handle it when lat/lon are set to nan.  I frankly have no idea why there is orbital data for both the JPL and RSS products that have nan for the geolocation.  That isn't normal.  But, okay, let's deal with it.  \n",
    "- stack the dataset scanline and cell positions into a new variable 'z'\n",
    "- drop all variables from the dataset when the longitude is nan\n",
    "- set up the tree\n",
    "- loop through the orbital data\n",
    "- only save a match if it is less than 0.25 deg distance AND time is less than any previous match\n",
    "- save the satellite indices & some basic data onto the USV grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_usv in range(8):\n",
    "    for isat in range(2):\n",
    "        ds_usv,usv_name = read_usv(num_usv)\n",
    "        if isat==0:\n",
    "            filelist = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'rss40km_filesave2.nc'\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'rss40km_usv2.nc'\n",
    "        if isat==1:\n",
    "            filelist = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'jplv4.2_filesave2.nc'   \n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'jplv42_usv2.nc'   \n",
    "        df = xr.open_dataset(filelist)\n",
    "        print(isat)\n",
    "        for file2 in df.filenames.data:\n",
    "            file = file2\n",
    "            file.replace('\\\\','/')\n",
    "            ds = xr.open_dataset(file)\n",
    "            ds.close()  \n",
    "            if isat==0:  #change RSS data to conform with JPL definitions\n",
    "                ds = ds.isel(look=0)\n",
    "                ds = ds.rename({'iqc_flag':'quality_flag','cellon':'lon','cellat':'lat','sss_smap':'smap_sss','ydim_grid':'phony_dim_0','xdim_grid':'phony_dim_1'})\n",
    "                ds['lon']=np.mod(ds.lon+180,360)-180  \n",
    "            if isat==1:  #change RSS data to conform with JPL definitions\n",
    "                ds = ds.rename({'row_time':'time'})\n",
    "            #stack xarray dataset then drop lon == nan\n",
    "            ds2 = ds.stack(z=('phony_dim_0', 'phony_dim_1')).reset_index('z')\n",
    "            #drop nan\n",
    "            ds_drop = ds2.where(np.isfinite(ds2.lon),drop=True)\n",
    "            lats = ds_drop.lat.data\n",
    "            lons = ds_drop.lon.data\n",
    "            inputdata = list(zip(lons.ravel(), lats.ravel()))\n",
    "            tree = spatial.KDTree(inputdata)\n",
    "            orbit_time = ds.time.max().data-np.timedelta64(1,'D')\n",
    "            orbit_time2 = ds.time.max().data+np.timedelta64(1,'D')    \n",
    "            usv_subset = ds_usv.sel(time=slice(orbit_time,orbit_time2))\n",
    "            ilen = ds_usv.time.size\n",
    "            for iusv in range(ilen):\n",
    "                if (ds_usv.time[iusv]<orbit_time) or (ds_usv.time[iusv]>orbit_time2):\n",
    "                    continue\n",
    "                pts = np.array([ds_usv.lon[iusv], ds_usv.lat[iusv]])\n",
    "        #        pts = np.array([ds_usv.lon[iusv]+360, ds_usv.lat[iusv]])\n",
    "                tree.query(pts,k=1)\n",
    "                i = tree.query(pts)[1]\n",
    "                rdist = tree.query(pts)[0]\n",
    "                #don't use matchups more than 25 km away\n",
    "                if rdist>.25:\n",
    "                    continue\n",
    "                #use .where to find the original indices of the matched data point\n",
    "                #find by matching sss and lat, just randomly chosen variables, you could use any\n",
    "                result = np.where((ds.smap_sss == ds_drop.smap_sss[i].data) & (ds.lat == ds_drop.lat[i].data))\n",
    "                listOfCoordinates = list(zip(result[0], result[1]))\n",
    "                if len(listOfCoordinates)==0:\n",
    "                    continue\n",
    "                ii, jj = listOfCoordinates[0][0],listOfCoordinates[0][1]\n",
    "                if isat==0:\n",
    "                    deltaTa = ((ds_usv.time[iusv]-ds.time[ii,jj]).data)/ np.timedelta64(1,'m')\n",
    "                if isat==1:\n",
    "                    deltaTa = ((ds_usv.time[iusv]-ds.time[ii]).data)/ np.timedelta64(1,'m')\n",
    "                if np.abs(deltaTa)<np.abs(ds_usv.deltaT[iusv].data):\n",
    "                    ds_usv.deltaT[iusv]=deltaTa\n",
    "                    ds_usv.smap_SSS[iusv]=ds.smap_sss[ii,jj]\n",
    "                    ds_usv.smap_iqc_flag[iusv]=ds.quality_flag[ii,jj]\n",
    "                    ds_usv.smap_name[iusv]=file2\n",
    "                    ds_usv.smap_dist[iusv]=rdist\n",
    "                    ds_usv.smap_ydim[iusv]=ii\n",
    "                    ds_usv.smap_xdim[iusv]=jj\n",
    "        ds_usv.to_netcdf(fileout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_usv in range(7):\n",
    "    for isat in range(2):`\n",
    "        ds_usv,usv_name = read_usv(num_usv)\n",
    "        if isat==0:\n",
    "            file = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'_rss40km_usv2.nc'\n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'_rss40km_usv2_norepeats.nc'\n",
    "        if isat==1:\n",
    "            file = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'_jplv42_usv2.nc'   \n",
    "            fileout = 'F:/data/cruise_data/saildrone/sat_collocations/'+usv_name+'_jplv42_usv2_norepeats.nc'   \n",
    "        ds_usv=xr.open_dataset(file)\n",
    "        ds_usv.close()\n",
    "        ds_usv = ds_usv.where(ds_usv.smap_SSS<10000,np.nan)\n",
    "        ilen,index = ds_usv.dims['time'],0\n",
    "        ds_tem = ds_usv.copy(deep=True)\n",
    "        duu, duu2, duv1, duv2, dlat, dlon, dut = [],[],[],[],[],[],np.empty((),dtype='datetime64')\n",
    "        index=0\n",
    "        while index <= ilen-2:\n",
    "            index += 1\n",
    "            if np.isnan(ds_usv.smap_SSS[index]):\n",
    "                continue\n",
    "            if np.isnan(ds_usv.smap_xdim[index]):\n",
    "                continue\n",
    "            result = np.where((ds_usv.smap_xdim == ds_tem.smap_xdim[index].data) & (ds_usv.smap_ydim == ds_tem.smap_ydim[index].data))       \n",
    "            duu=np.append(duu,ds_usv.smap_SSS[result[0][0]].data)\n",
    "            duu2=np.append(duu2,ds_usv.smap_iqc_flag[result[0][0]].data)\n",
    "            duv1=np.append(duv1,ds_usv.SAL_MEAN[result].mean().data)\n",
    "            dlat=np.append(dlat,ds_usv.lat[result].mean().data)\n",
    "            dlon=np.append(dlon,ds_usv.lon[result].mean().data)\n",
    "            dut=np.append(dut,ds_usv.time[result].mean().data)\n",
    "            ds_usv.smap_SSS[result]=np.nan\n",
    "        dut2 = dut[1:]  #remove first data point which is a repeat from what array defined       \n",
    "        ds_new=xr.Dataset(data_vars={'smap_SSS': ('time',duu),'smap_iqc_flag': ('time',duu2),\n",
    "                                     'SAL_MEAN':('time',duv1),\n",
    "                                     'lon': ('time',dlon),\n",
    "                                     'lat': ('time',dlat)},\n",
    "                          coords={'time':dut2})\n",
    "        ds_new.to_netcdf(fileout)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
